{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leçon 3 : La mémoire du modèle\n",
    "\n",
    "## Le problème de la mémoire courte\n",
    "\n",
    "Dans les leçons précédentes, notre modèle ne regardait que la **dernière lettre**.\n",
    "C'est comme essayer de deviner la fin d'une phrase en n'écoutant que le dernier mot.\n",
    "\n",
    "Exemple : après la lettre 'a', le modèle ne sait pas si on est dans\n",
    "\"**cla**ra\" ou \"**a**dam\" -- pourtant la suite est très différente !\n",
    "\n",
    "Solution : donner une **mémoire** au modèle. On appelle ça les **embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les embeddings : transformer des lettres en nombres\n",
    "\n",
    "L'idée : chaque lettre est représentée par une **liste de nombres** (un vecteur).\n",
    "\n",
    "Par exemple :\n",
    "- 'a' -> [0.3, -0.1, 0.8]\n",
    "- 'b' -> [-0.5, 0.4, 0.2]\n",
    "\n",
    "Ces nombres ne sont pas choisis à la main : le modèle les **apprend** pendant\n",
    "l'entraînement. Les lettres qui se comportent de façon similaire auront\n",
    "des nombres proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Notre alphabet\n",
    "alphabet = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "char_to_id = {c: i for i, c in enumerate(alphabet)}\n",
    "id_to_char = {i: c for i, c in enumerate(alphabet)}\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "print(f\"Taille du vocabulaire : {vocab_size} caractères\")\n",
    "print(f\"Exemples : 'a' = {char_to_id['a']}, 'z' = {char_to_id['z']}, '.' = {char_to_id['.']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créons les embeddings : chaque lettre = un vecteur de taille 8\n",
    "EMBED_DIM = 8\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Initialisation aléatoire (le modèle apprendra les bonnes valeurs)\n",
    "embeddings = [\n",
    "    [random.gauss(0, 0.5) for _ in range(EMBED_DIM)]\n",
    "    for _ in range(vocab_size)\n",
    "]\n",
    "\n",
    "print(f\"Embedding de 'a' : {[f'{x:.2f}' for x in embeddings[char_to_id['a']]]}\")\n",
    "print(f\"Embedding de 'b' : {[f'{x:.2f}' for x in embeddings[char_to_id['b']]]}\")\n",
    "print()\n",
    "print(\"Pour l'instant ces nombres sont aléatoires.\")\n",
    "print(\"Après entraînement, les lettres similaires auront des vecteurs proches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regarder plusieurs lettres en arrière\n",
    "\n",
    "Maintenant, au lieu de regarder 1 seule lettre, on va regarder les\n",
    "**3 dernières lettres** (notre \"fenêtre de contexte\").\n",
    "\n",
    "On **concatène** (met bout à bout) leurs embeddings pour avoir une image\n",
    "complète du contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3  # On regarde 3 lettres en arrière\n",
    "\n",
    "def get_context_vector(mot, position, embeddings):\n",
    "    \"\"\"Récupère les embeddings des 3 dernières lettres et les concatène.\"\"\"\n",
    "    vecteur = []\n",
    "    for i in range(CONTEXT_SIZE):\n",
    "        pos = position - CONTEXT_SIZE + i\n",
    "        if pos < 0:\n",
    "            # Avant le début du mot, on utilise le padding (.)\n",
    "            char_id = char_to_id['.']\n",
    "        else:\n",
    "            char_id = char_to_id[mot[pos]]\n",
    "        vecteur.extend(embeddings[char_id])\n",
    "    return vecteur\n",
    "\n",
    "# Exemple : pour prédire la 4e lettre de \"hugo\"\n",
    "mot = \".hugo.\"\n",
    "position = 4  # on veut prédire 'o' (position 4)\n",
    "contexte = get_context_vector(mot, position, embeddings)\n",
    "\n",
    "print(f\"Mot : '{mot}'\")\n",
    "print(f\"Pour prédire la lettre en position {position} ('{mot[position]}'),\")\n",
    "print(f\"on regarde les 3 lettres précédentes : '{mot[position-3:position]}'\")\n",
    "print(f\"Vecteur de contexte : {len(contexte)} nombres ({CONTEXT_SIZE} x {EMBED_DIM})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un mini réseau de neurones\n",
    "\n",
    "On prend le vecteur de contexte et on le passe dans un **réseau de neurones**\n",
    "simple (une seule couche) pour obtenir les probabilités de chaque lettre.\n",
    "\n",
    "```\n",
    "[contexte: 24 nombres] --> [couche: multiplication + addition] --> [27 scores] --> [probas]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = CONTEXT_SIZE * EMBED_DIM  # 3 * 8 = 24\n",
    "\n",
    "# Les poids de notre couche (une matrice 24 x 27)\n",
    "W = [[random.gauss(0, 0.3) for _ in range(vocab_size)] for _ in range(INPUT_DIM)]\n",
    "b = [0.0] * vocab_size  # biais\n",
    "\n",
    "def forward(contexte, W, b):\n",
    "    \"\"\"Passe le contexte dans le réseau pour obtenir des scores.\"\"\"\n",
    "    scores = list(b)  # copie du biais\n",
    "    for j in range(vocab_size):\n",
    "        for i in range(INPUT_DIM):\n",
    "            scores[j] += contexte[i] * W[i][j]\n",
    "    return scores\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilités (entre 0 et 1, somme = 1).\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "# Test\n",
    "scores = forward(contexte, W, b)\n",
    "probas = softmax(scores)\n",
    "\n",
    "# Top 5 prédictions pour la lettre après 'hug' dans 'hugo'\n",
    "top5 = sorted(range(vocab_size), key=lambda i: -probas[i])[:5]\n",
    "print(\"Prédictions (avant entraînement) pour la lettre après 'hug' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"\\n  (C'est du hasard pour l'instant - il faut entraîner !)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "prenoms = [\n",
    "    \"emma\", \"lucas\", \"lea\", \"hugo\", \"chloe\",\n",
    "    \"louis\", \"alice\", \"jules\", \"lina\", \"adam\",\n",
    "    \"rose\", \"arthur\", \"manon\", \"paul\", \"jade\",\n",
    "    \"nathan\", \"eva\", \"leo\", \"clara\", \"noah\",\n",
    "]\n",
    "\n",
    "vitesse = 0.01\n",
    "\n",
    "print(\"Entraînement avec contexte de 3 lettres...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "\n",
    "    for prenom in prenoms:\n",
    "        mot = \".\" + prenom + \".\"\n",
    "        for pos in range(1, len(mot)):\n",
    "            cible = char_to_id[mot[pos]]\n",
    "\n",
    "            # Forward\n",
    "            ctx = get_context_vector(mot, pos, embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "\n",
    "            # Loss\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "\n",
    "            # Gradient simplifié pour W et b\n",
    "            for j in range(vocab_size):\n",
    "                grad = probas[j] - (1 if j == cible else 0)\n",
    "                b[j] -= vitesse * grad\n",
    "                for i in range(INPUT_DIM):\n",
    "                    W[i][j] -= vitesse * grad * ctx[i]\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")\n",
    "\n",
    "print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer avec le modèle entraîné\n",
    "def generer(n=10):\n",
    "    resultats = []\n",
    "    for _ in range(n):\n",
    "        mot = \".\"\n",
    "        for _ in range(20):\n",
    "            ctx = get_context_vector(mot, len(mot), embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "            idx = random.choices(range(vocab_size), weights=probas, k=1)[0]\n",
    "            if idx == char_to_id['.']:\n",
    "                break\n",
    "            mot += id_to_char[idx]\n",
    "        if len(mot) > 1:\n",
    "            resultats.append(mot[1:].capitalize())\n",
    "    return resultats\n",
    "\n",
    "print(\"Prénoms générés (avec contexte de 3 lettres) :\")\n",
    "print()\n",
    "for p in generer(10):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print()\n",
    "print(\"Mieux qu'avant ! Le modèle 'comprend' des combinaisons de lettres.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce qu'on a appris\n",
    "\n",
    "- Les **embeddings** transforment des lettres en nombres que le modèle peut manipuler\n",
    "- Un **contexte** plus large (3 lettres au lieu de 1) donne de meilleurs résultats\n",
    "- Un **réseau de neurones** (même simple) combine le contexte pour faire des prédictions\n",
    "\n",
    "### Et ensuite ?\n",
    "\n",
    "Notre modèle regarde toujours une fenêtre fixe de 3 lettres. Et s'il pouvait\n",
    "**choisir** quelles lettres sont importantes, même si elles sont loin ?\n",
    "C'est exactement ce que fait le **mécanisme d'attention** -- le cœur des GPT.\n",
    "\n",
    "---\n",
    "*Prochaine leçon : [04 - L'attention](04_lattention.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Sources (ISO 42001)\n\n- **Embeddings et réseau feed-forward** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, section token/position embeddings\n- **Architecture du contexte par concaténation** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n- **Concept d'embedding spaces** : [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) — Grant Sanderson",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}