{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecon 3 : La memoire du modele\n",
    "\n",
    "## Le probleme de la memoire courte\n",
    "\n",
    "Dans les lecons precedentes, notre modele ne regardait que la **derniere lettre**.\n",
    "C'est comme essayer de deviner la fin d'une phrase en n'ecoutant que le dernier mot.\n",
    "\n",
    "Exemple : apres la lettre 'a', le modele ne sait pas si on est dans\n",
    "\"**cla**ra\" ou \"**a**dam\" -- pourtant la suite est tres differente !\n",
    "\n",
    "Solution : donner une **memoire** au modele. On appelle ca les **embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les embeddings : transformer des lettres en nombres\n",
    "\n",
    "L'idee : chaque lettre est representee par une **liste de nombres** (un vecteur).\n",
    "\n",
    "Par exemple :\n",
    "- 'a' -> [0.3, -0.1, 0.8]\n",
    "- 'b' -> [-0.5, 0.4, 0.2]\n",
    "\n",
    "Ces nombres ne sont pas choisis a la main : le modele les **apprend** pendant\n",
    "l'entrainement. Les lettres qui se comportent de facon similaire auront\n",
    "des nombres proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Notre alphabet\n",
    "alphabet = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "char_to_id = {c: i for i, c in enumerate(alphabet)}\n",
    "id_to_char = {i: c for i, c in enumerate(alphabet)}\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "print(f\"Taille du vocabulaire : {vocab_size} caracteres\")\n",
    "print(f\"Exemples : 'a' = {char_to_id['a']}, 'z' = {char_to_id['z']}, '.' = {char_to_id['.']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creons les embeddings : chaque lettre = un vecteur de taille 8\n",
    "EMBED_DIM = 8\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Initialisation aleatoire (le modele apprendra les bonnes valeurs)\n",
    "embeddings = [\n",
    "    [random.gauss(0, 0.5) for _ in range(EMBED_DIM)]\n",
    "    for _ in range(vocab_size)\n",
    "]\n",
    "\n",
    "print(f\"Embedding de 'a' : {[f'{x:.2f}' for x in embeddings[char_to_id['a']]]}\")\n",
    "print(f\"Embedding de 'b' : {[f'{x:.2f}' for x in embeddings[char_to_id['b']]]}\")\n",
    "print()\n",
    "print(\"Pour l'instant ces nombres sont aleatoires.\")\n",
    "print(\"Apres entrainement, les lettres similaires auront des vecteurs proches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regarder plusieurs lettres en arriere\n",
    "\n",
    "Maintenant, au lieu de regarder 1 seule lettre, on va regarder les\n",
    "**3 dernieres lettres** (notre \"fenetre de contexte\").\n",
    "\n",
    "On **concatene** (met bout a bout) leurs embeddings pour avoir une image\n",
    "complete du contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3  # On regarde 3 lettres en arriere\n",
    "\n",
    "def get_context_vector(mot, position, embeddings):\n",
    "    \"\"\"Recupere les embeddings des 3 dernieres lettres et les concatene.\"\"\"\n",
    "    vecteur = []\n",
    "    for i in range(CONTEXT_SIZE):\n",
    "        pos = position - CONTEXT_SIZE + i\n",
    "        if pos < 0:\n",
    "            # Avant le debut du mot, on utilise le padding (.)\n",
    "            char_id = char_to_id['.']\n",
    "        else:\n",
    "            char_id = char_to_id[mot[pos]]\n",
    "        vecteur.extend(embeddings[char_id])\n",
    "    return vecteur\n",
    "\n",
    "# Exemple : pour predire la 4e lettre de \"hugo\"\n",
    "mot = \".hugo.\"\n",
    "position = 4  # on veut predire 'o' (position 4)\n",
    "contexte = get_context_vector(mot, position, embeddings)\n",
    "\n",
    "print(f\"Mot : '{mot}'\")\n",
    "print(f\"Pour predire la lettre en position {position} ('{mot[position]}'),\")\n",
    "print(f\"on regarde les 3 lettres precedentes : '{mot[position-3:position]}'\")\n",
    "print(f\"Vecteur de contexte : {len(contexte)} nombres ({CONTEXT_SIZE} x {EMBED_DIM})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un mini reseau de neurones\n",
    "\n",
    "On prend le vecteur de contexte et on le passe dans un **reseau de neurones**\n",
    "simple (une seule couche) pour obtenir les probabilites de chaque lettre.\n",
    "\n",
    "```\n",
    "[contexte: 24 nombres] --> [couche: multiplication + addition] --> [27 scores] --> [probas]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = CONTEXT_SIZE * EMBED_DIM  # 3 * 8 = 24\n",
    "\n",
    "# Les poids de notre couche (une matrice 24 x 27)\n",
    "W = [[random.gauss(0, 0.3) for _ in range(vocab_size)] for _ in range(INPUT_DIM)]\n",
    "b = [0.0] * vocab_size  # biais\n",
    "\n",
    "def forward(contexte, W, b):\n",
    "    \"\"\"Passe le contexte dans le reseau pour obtenir des scores.\"\"\"\n",
    "    scores = list(b)  # copie du biais\n",
    "    for j in range(vocab_size):\n",
    "        for i in range(INPUT_DIM):\n",
    "            scores[j] += contexte[i] * W[i][j]\n",
    "    return scores\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilites (entre 0 et 1, somme = 1).\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "# Test\n",
    "scores = forward(contexte, W, b)\n",
    "probas = softmax(scores)\n",
    "\n",
    "# Top 5 predictions pour la lettre apres 'hug' dans 'hugo'\n",
    "top5 = sorted(range(vocab_size), key=lambda i: -probas[i])[:5]\n",
    "print(\"Predictions (avant entrainement) pour la lettre apres 'hug' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"\\n  (C'est du hasard pour l'instant - il faut entrainer !)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "prenoms = [\n",
    "    \"emma\", \"lucas\", \"lea\", \"hugo\", \"chloe\",\n",
    "    \"louis\", \"alice\", \"jules\", \"lina\", \"adam\",\n",
    "    \"rose\", \"arthur\", \"manon\", \"paul\", \"jade\",\n",
    "    \"nathan\", \"eva\", \"leo\", \"clara\", \"noah\",\n",
    "]\n",
    "\n",
    "vitesse = 0.01\n",
    "\n",
    "print(\"Entrainement avec contexte de 3 lettres...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "\n",
    "    for prenom in prenoms:\n",
    "        mot = \".\" + prenom + \".\"\n",
    "        for pos in range(1, len(mot)):\n",
    "            cible = char_to_id[mot[pos]]\n",
    "\n",
    "            # Forward\n",
    "            ctx = get_context_vector(mot, pos, embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "\n",
    "            # Loss\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "\n",
    "            # Gradient simplifie pour W et b\n",
    "            for j in range(vocab_size):\n",
    "                grad = probas[j] - (1 if j == cible else 0)\n",
    "                b[j] -= vitesse * grad\n",
    "                for i in range(INPUT_DIM):\n",
    "                    W[i][j] -= vitesse * grad * ctx[i]\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")\n",
    "\n",
    "print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer avec le modele entraine\n",
    "def generer(n=10):\n",
    "    resultats = []\n",
    "    for _ in range(n):\n",
    "        mot = \".\"\n",
    "        for _ in range(20):\n",
    "            ctx = get_context_vector(mot, len(mot), embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "            idx = random.choices(range(vocab_size), weights=probas, k=1)[0]\n",
    "            if idx == char_to_id['.']:\n",
    "                break\n",
    "            mot += id_to_char[idx]\n",
    "        if len(mot) > 1:\n",
    "            resultats.append(mot[1:].capitalize())\n",
    "    return resultats\n",
    "\n",
    "print(\"Prenoms generes (avec contexte de 3 lettres) :\")\n",
    "print()\n",
    "for p in generer(10):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print()\n",
    "print(\"Mieux qu'avant ! Le modele 'comprend' des combinaisons de lettres.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce qu'on a appris\n",
    "\n",
    "- Les **embeddings** transforment des lettres en nombres que le modele peut manipuler\n",
    "- Un **contexte** plus large (3 lettres au lieu de 1) donne de meilleurs resultats\n",
    "- Un **reseau de neurones** (meme simple) combine le contexte pour faire des predictions\n",
    "\n",
    "### Et ensuite ?\n",
    "\n",
    "Notre modele regarde toujours une fenetre fixe de 3 lettres. Et s'il pouvait\n",
    "**choisir** quelles lettres sont importantes, meme si elles sont loin ?\n",
    "C'est exactement ce que fait le **mecanisme d'attention** -- le coeur des GPT.\n",
    "\n",
    "---\n",
    "*Prochaine lecon : [04 - L'attention](04_lattention.ipynb)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
