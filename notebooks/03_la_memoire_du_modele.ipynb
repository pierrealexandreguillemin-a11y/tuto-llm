{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Rappel** : clique sur une cellule grise, puis **Shift + Entree** pour l'executer.\n> Execute les cellules **dans l'ordre** de haut en bas.\n\n---\n\n# Leçon 3 : La mémoire du modèle\n\n## Le problème de la mémoire courte\n\nDans les leçons précédentes, notre modèle ne regardait que la **dernière lettre**.\nC'est comme essayer de deviner la fin d'une phrase en n'écoutant que le dernier mot.\n\nExemple : après les lettres 'salame', le modèle ne sait pas si on est dans\n\"**salame**che\" ou \"**salame**nce\" -- pourtant la suite est très différente !\n\nSolution : donner une **mémoire** au modèle. On appelle ça les **embeddings**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Les embeddings : transformer des lettres en nombres\n\nL'idée : chaque lettre est représentée par une **liste de nombres** (un vecteur).\n\nPar exemple :\n- 'a' -> [0.3, -0.1, 0.8]\n- 'b' -> [-0.5, 0.4, 0.2]\n\nCes nombres ne sont pas choisis à la main : le modèle les **apprend** pendant\nl'entraînement. Les lettres qui se comportent de façon similaire auront\ndes nombres proches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# Notre alphabet\n",
    "alphabet = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "char_to_id = {c: i for i, c in enumerate(alphabet)}\n",
    "id_to_char = {i: c for i, c in enumerate(alphabet)}\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "print(f\"Taille du vocabulaire : {vocab_size} caractères\")\n",
    "print(\n",
    "    f\"Exemples : 'a' = {char_to_id['a']}, 'z' = {char_to_id['z']}, '.' = {char_to_id['.']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Créons les embeddings : chaque lettre = un vecteur de taille EMBED_DIM\nEMBED_DIM = 8\n\nrandom.seed(42)\n\n# Initialisation aléatoire (le modèle apprendra les bonnes valeurs)\nembeddings = [\n    [random.gauss(0, 0.5) for _ in range(EMBED_DIM)] for _ in range(vocab_size)\n]\n\nprint(f\"Embedding de 'a' : {[f'{x:.2f}' for x in embeddings[char_to_id['a']]]}\")\nprint(f\"Embedding de 'b' : {[f'{x:.2f}' for x in embeddings[char_to_id['b']]]}\")\nprint()\nprint(\"Pour l'instant ces nombres sont aléatoires.\")\nprint(\"Après entraînement, les lettres similaires auront des vecteurs proches.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 1)\n\nDans la cellule ci-dessous, change `EMBED_DIM_test` pour voir comment\nla taille de l'embedding change la taille du vecteur de contexte.\nAvec `4`, chaque lettre est decrite par 4 nombres. Avec `16`, par 16 !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 1 : Change la dimension d'embedding, puis Shift + Entree ---\n",
    "EMBED_DIM_test = 8  # <-- Essaie 4 (petit) ou 16 (grand) !\n",
    "\n",
    "print(f\"Avec EMBED_DIM = {EMBED_DIM_test} :\")\n",
    "print(f\"  Chaque lettre = {EMBED_DIM_test} nombres\")\n",
    "print(f\"  3 lettres de contexte = 3 x {EMBED_DIM_test} = {3 * EMBED_DIM_test} nombres\")\n",
    "if EMBED_DIM_test <= 4:\n",
    "    print(\"  -> Petit : le modele a peu d'information sur chaque lettre.\")\n",
    "elif EMBED_DIM_test >= 16:\n",
    "    print(\"  -> Grand : plus d'information, mais plus de calculs !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Regarder plusieurs lettres en arrière\n\nMaintenant, au lieu de regarder 1 seule lettre, on va regarder les\n**3 dernières lettres** (notre \"fenêtre de contexte\").\n\nOn **concatène** (met bout à bout) leurs embeddings pour avoir une image\ncomplète du contexte."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONTEXT_SIZE = 3\n\n\ndef get_context_vector(mot, position, embeddings):\n    \"\"\"Récupère les embeddings des 3 dernières lettres et les concatène.\"\"\"\n    vecteur = []\n    for i in range(CONTEXT_SIZE):\n        pos = position - CONTEXT_SIZE + i\n        if pos < 0:\n            # Avant le début du mot, on utilise le padding (.)\n            char_id = char_to_id[\".\"]\n        else:\n            char_id = char_to_id[mot[pos]]\n        vecteur.extend(embeddings[char_id])\n    return vecteur\n\n\n# Exemple : pour prédire la 5e lettre de \"pikachu\"\nmot = \".pikachu.\"\nposition = 4  # on veut prédire 'a' (position 4)\ncontexte = get_context_vector(mot, position, embeddings)\n\nprint(f\"Mot : '{mot}'\")\nprint(f\"Pour prédire la lettre en position {position} ('{mot[position]}'),\")\nprint(\n    f\"on regarde les {CONTEXT_SIZE} lettres précédentes : '{mot[max(0, position - CONTEXT_SIZE) : position]}'\"\n)\nprint(f\"Vecteur de contexte : {len(contexte)} nombres ({CONTEXT_SIZE} x {EMBED_DIM})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 2)\n\nDans la cellule ci-dessous, change `context_test` pour comparer\ndifferentes tailles de memoire. Avec `1`, c'est comme la lecon 2 !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 2 : Change la taille du contexte, puis Shift + Entree ---\n",
    "context_test = 3  # <-- Essaie 1 (comme la lecon 2) ou 5 (plus de memoire) !\n",
    "\n",
    "taille_vecteur = context_test * EMBED_DIM\n",
    "print(f\"Avec CONTEXT_SIZE = {context_test} :\")\n",
    "print(f\"  Le modele regarde {context_test} lettre(s) en arriere\")\n",
    "print(\n",
    "    f\"  Vecteur de contexte = {context_test} x {EMBED_DIM} = {taille_vecteur} nombres\"\n",
    ")\n",
    "if context_test == 1:\n",
    "    print(\"  -> Pareil que la lecon 2 : 1 seule lettre !\")\n",
    "elif context_test >= 5:\n",
    "    print(\"  -> Beaucoup de memoire, mais plus long a entrainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Un mini réseau de neurones\n\nOn prend le vecteur de contexte et on le passe dans un **réseau de neurones**\nsimple (une seule couche) pour obtenir les probabilités de chaque lettre.\n\n```\n[contexte: 24 nombres] --> [couche: multiplication + addition] --> [27 scores] --> [probas]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = CONTEXT_SIZE * EMBED_DIM  # 3 * 8 = 24\n",
    "\n",
    "# Les poids de notre couche (une matrice 24 x 27)\n",
    "W = [[random.gauss(0, 0.3) for _ in range(vocab_size)] for _ in range(INPUT_DIM)]\n",
    "b = [0.0] * vocab_size  # biais\n",
    "\n",
    "\n",
    "def forward(contexte, W, b):\n",
    "    \"\"\"Passe le contexte dans le réseau pour obtenir des scores.\"\"\"\n",
    "    scores = list(b)  # copie du biais\n",
    "    for j in range(vocab_size):\n",
    "        for i in range(INPUT_DIM):\n",
    "            scores[j] += contexte[i] * W[i][j]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilités (entre 0 et 1, somme = 1).\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "\n",
    "# Test\n",
    "scores = forward(contexte, W, b)\n",
    "probas = softmax(scores)\n",
    "\n",
    "# Top 5 prédictions pour la lettre après 'pik' dans 'pikachu'\n",
    "top5 = sorted(range(vocab_size), key=lambda i: -probas[i])[:5]\n",
    "print(\"Prédictions (avant entraînement) pour la lettre après 'pik' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"\\n  (C'est du hasard pour l'instant - il faut entraîner !)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "pokemons = [\n",
    "    \"arcanin\",\n",
    "    \"bulbizarre\",\n",
    "    \"carapuce\",\n",
    "    \"dracaufeu\",\n",
    "    \"ectoplasma\",\n",
    "    \"evoli\",\n",
    "    \"felinferno\",\n",
    "    \"gardevoir\",\n",
    "    \"goupix\",\n",
    "    \"lokhlass\",\n",
    "    \"lucario\",\n",
    "    \"metamorph\",\n",
    "    \"mewtwo\",\n",
    "    \"noctali\",\n",
    "    \"pikachu\",\n",
    "    \"rondoudou\",\n",
    "    \"ronflex\",\n",
    "    \"salameche\",\n",
    "    \"togepi\",\n",
    "    \"voltali\",\n",
    "]\n",
    "\n",
    "vitesse = 0.01\n",
    "\n",
    "print(\"Entraînement avec contexte de 3 lettres...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "\n",
    "    for pokemon in pokemons:\n",
    "        mot = \".\" + pokemon + \".\"\n",
    "        for pos in range(1, len(mot)):\n",
    "            cible = char_to_id[mot[pos]]\n",
    "\n",
    "            # Forward\n",
    "            ctx = get_context_vector(mot, pos, embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "\n",
    "            # Loss\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "\n",
    "            # Gradient simplifié pour W et b\n",
    "            for j in range(vocab_size):\n",
    "                grad = probas[j] - (1 if j == cible else 0)\n",
    "                b[j] -= vitesse * grad\n",
    "                for i in range(INPUT_DIM):\n",
    "                    W[i][j] -= vitesse * grad * ctx[i]\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")\n",
    "\n",
    "print(f\"  Epoch {epoch:3d} | Loss : {loss_totale / nb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer avec le modèle entraîné\n",
    "def generer(n=10):\n",
    "    resultats = []\n",
    "    for _ in range(n):\n",
    "        mot = \".\"\n",
    "        for _ in range(20):\n",
    "            ctx = get_context_vector(mot, len(mot), embeddings)\n",
    "            scores = forward(ctx, W, b)\n",
    "            probas = softmax(scores)\n",
    "            idx = random.choices(range(vocab_size), weights=probas, k=1)[0]\n",
    "            if idx == char_to_id[\".\"]:\n",
    "                break\n",
    "            mot += id_to_char[idx]\n",
    "        if len(mot) > 1:\n",
    "            resultats.append(mot[1:].capitalize())\n",
    "    return resultats\n",
    "\n",
    "\n",
    "print(\"Pokémon générés (avec contexte de 3 lettres) :\")\n",
    "print()\n",
    "for p in generer(10):\n",
    "    print(f\"  {p}\")\n",
    "\n",
    "print()\n",
    "print(\"Mieux qu'avant ! Le modèle 'comprend' des combinaisons de lettres.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 3)\n\nDans la cellule ci-dessous, change le nombre pour generer **30** noms.\nCompare avec les noms de la lecon 2 : est-ce que ceux-ci sont meilleurs ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 3 : Change le nombre, puis Shift + Entree ---\n",
    "nombre = 10  # <-- Mets 30 ici !\n",
    "\n",
    "print(f\"Generation de {nombre} Pokemon :\")\n",
    "print()\n",
    "for i, nom in enumerate(generer(nombre)):\n",
    "    print(f\"  {i + 1}. {nom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Ce qu'on a appris\n\n- Les **embeddings** transforment des lettres en nombres que le modèle peut manipuler\n- Un **contexte** plus large (3 lettres au lieu de 1) donne de meilleurs résultats\n- Un **réseau de neurones** (même simple) combine le contexte pour faire des prédictions\n\n### Et ensuite ?\n\nNotre modèle regarde toujours une fenêtre fixe de 3 lettres. Et s'il pouvait\n**choisir** quelles lettres sont importantes, même si elles sont loin ?\nC'est exactement ce que fait le **mécanisme d'attention** -- le cœur des GPT.\n\n---\n*Prochaine leçon : [04 - L'attention](04_lattention.ipynb)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources (ISO 42001)\n",
    "\n",
    "- **Embeddings et réseau feed-forward** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, section token/position embeddings\n",
    "- **Architecture du contexte par concaténation** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n",
    "- **Concept d'embedding spaces** : [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) — Grant Sanderson\n",
    "- **Dataset Pokémon** : (c) Nintendo / Creatures Inc. / GAME FREAK inc., usage éducatif. Source : [PokéAPI](https://pokeapi.co/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
