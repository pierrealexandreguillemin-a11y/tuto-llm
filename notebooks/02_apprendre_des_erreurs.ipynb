{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Rappel** : clique sur une cellule grise, puis **Shift + Entree** pour l'executer.\n> Execute les cellules **dans l'ordre** de haut en bas.\n\n---\n\n# Leçon 2 : Apprendre de ses erreurs\n\n## Le secret de l'IA : se tromper, corriger, recommencer\n\nImagine que tu apprends à lancer une balle dans un panier :\n1. Tu lances -> tu rates à droite\n2. Tu corriges un peu à gauche\n3. Tu relances -> plus près !\n4. Tu continues jusqu'à marquer\n\nL'IA fait **exactement** pareil. Elle fait une prédiction, regarde si c'est\nbon, et ajuste. Ça s'appelle **l'entraînement**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Étape 1 : Mesurer l'erreur\n\nD'abord, il faut un moyen de dire **à quel point** le modèle s'est trompé.\nOn appelle ça la **loss** (perte en anglais).\n\n- Loss haute = le modèle se trompe beaucoup\n- Loss basse = le modèle devine bien"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Imaginons que le modèle prédit les probabilités suivantes\n",
    "# pour la lettre qui suit 'p' dans 'pikachu' :\n",
    "\n",
    "prediction = {\n",
    "    \"i\": 0.6,  # 60% -> bonne réponse !\n",
    "    \"a\": 0.2,  # 20%\n",
    "    \"e\": 0.15,  # 15%\n",
    "    \"o\": 0.05,  # 5%\n",
    "}\n",
    "\n",
    "# La bonne réponse est 'i'\n",
    "bonne_reponse = \"i\"\n",
    "\n",
    "# La loss = à quel point on est surpris par la bonne réponse\n",
    "# Si on avait dit 100% pour 'i', la surprise serait de 0 (parfait !)\n",
    "# Si on avait dit 1% pour 'i', la surprise serait énorme\n",
    "\n",
    "loss = -math.log(prediction[bonne_reponse])\n",
    "print(\n",
    "    f\"Le modèle donnait {prediction[bonne_reponse]:.0%} de chance à '{bonne_reponse}'\"\n",
    ")\n",
    "print(f\"Loss = {loss:.2f}\")\n",
    "print()\n",
    "\n",
    "# Comparons avec une mauvaise prédiction\n",
    "mauvaise_prediction = {\"i\": 0.05, \"a\": 0.7, \"e\": 0.2, \"o\": 0.05}\n",
    "loss_mauvaise = -math.log(mauvaise_prediction[bonne_reponse])\n",
    "print(\n",
    "    f\"Si le modèle n'avait donné que {mauvaise_prediction[bonne_reponse]:.0%} à '{bonne_reponse}'...\"\n",
    ")\n",
    "print(f\"Loss = {loss_mauvaise:.2f}  (beaucoup plus haut = beaucoup plus faux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n### A toi de jouer ! (Exercice 1)\n\nDans la cellule ci-dessous, change la valeur de `ma_prediction` pour voir\ncomment la loss reagit. Essaie `0.9` (tres confiant) et `0.01` (presque sur\nque c'est faux).\n\nPlus la prediction est bonne, plus la loss est **basse**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- EXERCICE 1 : Change la prediction, puis Shift + Entree ---\nma_prediction = 0.6  # <-- Change cette valeur ! Essaie 0.9 ou 0.01\n\nma_loss = -math.log(ma_prediction)\nprint(f\"Si le modele donne {ma_prediction:.0%} de chance a la bonne reponse :\")\nprint(f\"  Loss = {ma_loss:.2f}\")\nif ma_prediction > 0.8:\n    print(\"  -> Tres bien ! Le modele est confiant et a raison.\")\nelif ma_prediction < 0.1:\n    print(\"  -> Enorme loss ! Le modele s'est beaucoup trompe.\")\nelse:\n    print(\"  -> Moyen. Le modele peut encore s'ameliorer.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Étape 2 : Les poids du modèle\n\nUn modèle, c'est juste une collection de **nombres** (on les appelle des **poids**).\nCes nombres déterminent les prédictions.\n\nEntraîner = trouver les bons nombres."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# On crée un mini-modèle : juste des scores pour chaque paire de lettres\n",
    "# Au début, les scores sont aléatoires -> le modèle ne sait rien\n",
    "\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz.\")\n",
    "\n",
    "# Scores aléatoires (les \"poids\" du modèle)\n",
    "random.seed(42)\n",
    "poids = {}\n",
    "for a in alphabet:\n",
    "    poids[a] = {}\n",
    "    for b in alphabet:\n",
    "        poids[a][b] = random.uniform(-1, 1)\n",
    "\n",
    "\n",
    "def calculer_probas(poids, lettre):\n",
    "    \"\"\"Transforme les scores en probabilités (softmax).\"\"\"\n",
    "    scores = poids[lettre]\n",
    "    # L'exponentielle rend tous les scores positifs\n",
    "    exps = {b: math.exp(scores[b]) for b in scores}\n",
    "    total = sum(exps.values())\n",
    "    return {b: exps[b] / total for b in scores}\n",
    "\n",
    "\n",
    "# Au début, les probas sont quasi uniformes (le modèle devine au hasard)\n",
    "p = calculer_probas(poids, \".\")\n",
    "lettres_debut = sorted(p.items(), key=lambda x: -x[1])[:5]\n",
    "print(\"Au début, le modèle pense que les Pokémon commencent par :\")\n",
    "for lettre, prob in lettres_debut:\n",
    "    print(f\"  '{lettre}' : {prob:.1%}\")\n",
    "print(\"\\n  -> C'est n'importe quoi ! Il faut l'entraîner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Étape 3 : Entraînement\n\nL'algorithme est simple :\n1. Prendre un nom de Pokémon d'entraînement\n2. Le modèle fait sa prédiction\n3. On calcule la loss (l'erreur)\n4. On **ajuste les poids** pour réduire la loss\n5. Recommencer\n\nL'étape 4 s'appelle la **descente de gradient**. C'est comme ajuster ton\ntir au panier un petit peu à chaque essai."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pokemons = [\n    \"arcanin\",\n    \"bulbizarre\",\n    \"carapuce\",\n    \"dracaufeu\",\n    \"ectoplasma\",\n    \"evoli\",\n    \"felinferno\",\n    \"gardevoir\",\n    \"goupix\",\n    \"lokhlass\",\n    \"lucario\",\n    \"metamorph\",\n    \"mewtwo\",\n    \"noctali\",\n    \"pikachu\",\n    \"rondoudou\",\n    \"ronflex\",\n    \"salameche\",\n    \"togepi\",\n    \"voltali\",\n]\n\n# Vitesse d'apprentissage : de combien on ajuste à chaque fois\n# Trop grand = on dépasse, trop petit = on apprend trop lentement\nvitesse = 0.1  # <-- Change cette valeur ! Essaie 0.01 ou 0.5\nnb_epochs = 50  # <-- Change cette valeur ! Essaie 10 ou 200\n\nprint(\"Entraînement...\")\nprint()\n\nfor epoch in range(nb_epochs):\n    loss_totale = 0\n    nb = 0\n\n    for pokemon in pokemons:\n        mot = \".\" + pokemon + \".\"\n        for i in range(len(mot) - 1):\n            lettre = mot[i]\n            cible = mot[i + 1]\n\n            # 1. Prédiction\n            probas = calculer_probas(poids, lettre)\n\n            # 2. Loss\n            loss_totale += -math.log(probas[cible] + 1e-10)\n            nb += 1\n\n            # 3. Ajuster les poids (gradient simplifié)\n            for b in alphabet:\n                if b == cible:\n                    # La bonne réponse : augmenter son score\n                    poids[lettre][b] += vitesse * (1 - probas[b])\n                else:\n                    # Les mauvaises réponses : baisser leur score\n                    poids[lettre][b] -= vitesse * probas[b]\n\n    if epoch % 10 == 0:\n        print(f\"  Epoch {epoch:2d} | Loss moyenne : {loss_totale / nb:.3f}\")\n\nprint(f\"  Epoch {epoch:2d} | Loss moyenne : {loss_totale / nb:.3f}\")\nprint()\nprint(\"La loss baisse = le modèle s'améliore !\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n### A toi de jouer ! (Exercice 2)\n\nDans la cellule d'entrainement ci-dessus, change `vitesse` et `nb_epochs`,\npuis re-execute-la. Essaie :\n- `vitesse = 0.5` : que se passe-t-il ? (instabilite !)\n- `vitesse = 0.01` : est-ce que la loss descend aussi vite ?\n- `nb_epochs = 200` : est-ce que ca continue a s'ameliorer ?\n\nLa cellule ci-dessous fait un mini-test rapide pour comparer :",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- EXERCICE 2 : Compare differentes vitesses, puis Shift + Entree ---\nimport copy\n\nvitesse_test = 0.5  # <-- Change cette valeur ! Essaie 0.01, 0.1, 0.5, 2.0\n\n# On fait un mini-entrainement de 5 epochs avec cette vitesse\npoids_test = copy.deepcopy(poids)\nfor epoch in range(5):\n    loss_t = 0\n    nb_t = 0\n    for pokemon in pokemons:\n        mot = \".\" + pokemon + \".\"\n        for i in range(len(mot) - 1):\n            probas_t = calculer_probas(poids_test, mot[i])\n            loss_t += -math.log(probas_t[mot[i + 1]] + 1e-10)\n            nb_t += 1\n            for b in alphabet:\n                if b == mot[i + 1]:\n                    poids_test[mot[i]][b] += vitesse_test * (1 - probas_t[b])\n                else:\n                    poids_test[mot[i]][b] -= vitesse_test * probas_t[b]\n    print(f\"  Epoch {epoch} | vitesse={vitesse_test} | Loss : {loss_t / nb_t:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voyons maintenant ce que le modèle a appris :\n",
    "p = calculer_probas(poids, \".\")\n",
    "lettres_debut = sorted(p.items(), key=lambda x: -x[1])[:5]\n",
    "print(\"Après entraînement, les Pokémon commencent par :\")\n",
    "for lettre, prob in lettres_debut:\n",
    "    print(f\"  '{lettre}' : {prob:.1%}\")\n",
    "print()\n",
    "print(\"C'est plus logique ! (c, m, g, e, r sont des débuts courants)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générons des noms de Pokémon avec le modèle entraîné\n",
    "def generer(poids, n=10):\n",
    "    resultats = []\n",
    "    for _ in range(n):\n",
    "        pokemon = \"\"\n",
    "        lettre = \".\"\n",
    "        for _ in range(20):  # max 20 lettres\n",
    "            p = calculer_probas(poids, lettre)\n",
    "            choix = list(p.keys())\n",
    "            probs = list(p.values())\n",
    "            lettre = random.choices(choix, weights=probs, k=1)[0]\n",
    "            if lettre == \".\":\n",
    "                break\n",
    "            pokemon += lettre\n",
    "        if pokemon:\n",
    "            resultats.append(pokemon.capitalize())\n",
    "    return resultats\n",
    "\n",
    "\n",
    "print(\"Noms de Pokémon inventés après entraînement :\")\n",
    "for p in generer(poids, 10):\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n### A toi de jouer ! (Exercice 3)\n\nDans la cellule ci-dessous, change le nombre pour generer **50** noms.\nEst-ce que certains ressemblent a de vrais Pokemon ?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- EXERCICE 3 : Change le nombre, puis Shift + Entree ---\nnombre = 10  # <-- Mets 50 ici !\n\nprint(f\"Generation de {nombre} Pokemon :\")\nprint()\nfor i, nom in enumerate(generer(poids, nombre)):\n    print(f\"  {i + 1}. {nom}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Ce qu'on a appris\n\n- La **loss** mesure à quel point le modèle se trompe\n- Les **poids** sont les nombres que le modèle ajuste pour apprendre\n- L'**entraînement** = ajuster les poids pour réduire la loss, encore et encore\n- Même un modèle simple s'améliore avec l'entraînement !\n\n### Limite\n\nNotre modèle ne regarde encore que **1 lettre en arrière**.\nDans la prochaine leçon, on va lui donner une **mémoire** pour qu'il\nse souvienne de plusieurs lettres à la fois.\n\n---\n*Prochaine leçon : [03 - La mémoire du modèle](03_la_memoire_du_modele.ipynb)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources (ISO 42001)\n",
    "\n",
    "- **Cross-entropy loss et descente de gradient** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, lignes implémentant le backward pass\n",
    "- **Analogie du gradient comme correction** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n",
    "- **Visualisation de la descente de gradient** : [3Blue1Brown - Gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w) — Grant Sanderson\n",
    "- **Dataset Pokémon** : (c) Nintendo / Creatures Inc. / GAME FREAK inc., usage éducatif. Source : [PokéAPI](https://pokeapi.co/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
