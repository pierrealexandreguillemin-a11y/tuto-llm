{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leçon 2 : Apprendre de ses erreurs\n",
    "\n",
    "## Le secret de l'IA : se tromper, corriger, recommencer\n",
    "\n",
    "Imagine que tu apprends à lancer une balle dans un panier :\n",
    "1. Tu lances -> tu rates à droite\n",
    "2. Tu corriges un peu à gauche\n",
    "3. Tu relances -> plus près !\n",
    "4. Tu continues jusqu'à marquer\n",
    "\n",
    "L'IA fait **exactement** pareil. Elle fait une prédiction, regarde si c'est\n",
    "bon, et ajuste. Ça s'appelle **l'entraînement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 1 : Mesurer l'erreur\n",
    "\n",
    "D'abord, il faut un moyen de dire **à quel point** le modèle s'est trompé.\n",
    "On appelle ça la **loss** (perte en anglais).\n",
    "\n",
    "- Loss haute = le modèle se trompe beaucoup\n",
    "- Loss basse = le modèle devine bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Imaginons que le modèle prédit les probabilités suivantes\n",
    "# pour la lettre qui suit 'h' dans le prénom 'hugo' :\n",
    "\n",
    "prediction = {\n",
    "    'u': 0.6,   # 60% -> bonne réponse !\n",
    "    'a': 0.2,   # 20%\n",
    "    'e': 0.15,  # 15%\n",
    "    'o': 0.05,  # 5%\n",
    "}\n",
    "\n",
    "# La bonne réponse est 'u'\n",
    "bonne_reponse = 'u'\n",
    "\n",
    "# La loss = à quel point on est surpris par la bonne réponse\n",
    "# Si on avait dit 100% pour 'u', la surprise serait de 0 (parfait !)\n",
    "# Si on avait dit 1% pour 'u', la surprise serait énorme\n",
    "\n",
    "loss = -math.log(prediction[bonne_reponse])\n",
    "print(f\"Le modèle donnait {prediction[bonne_reponse]:.0%} de chance à '{bonne_reponse}'\")\n",
    "print(f\"Loss = {loss:.2f}\")\n",
    "print()\n",
    "\n",
    "# Comparons avec une mauvaise prédiction\n",
    "mauvaise_prediction = {'u': 0.05, 'a': 0.7, 'e': 0.2, 'o': 0.05}\n",
    "loss_mauvaise = -math.log(mauvaise_prediction[bonne_reponse])\n",
    "print(f\"Si le modèle n'avait donné que {mauvaise_prediction[bonne_reponse]:.0%} à '{bonne_reponse}'...\")\n",
    "print(f\"Loss = {loss_mauvaise:.2f}  (beaucoup plus haut = beaucoup plus faux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2 : Les poids du modèle\n",
    "\n",
    "Un modèle, c'est juste une collection de **nombres** (on les appelle des **poids**).\n",
    "Ces nombres déterminent les prédictions.\n",
    "\n",
    "Entraîner = trouver les bons nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# On crée un mini-modèle : juste des scores pour chaque paire de lettres\n",
    "# Au début, les scores sont aléatoires -> le modèle ne sait rien\n",
    "\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz.\")\n",
    "\n",
    "# Scores aléatoires (les \"poids\" du modèle)\n",
    "random.seed(42)\n",
    "poids = {}\n",
    "for a in alphabet:\n",
    "    poids[a] = {}\n",
    "    for b in alphabet:\n",
    "        poids[a][b] = random.uniform(-1, 1)\n",
    "\n",
    "def calculer_probas(poids, lettre):\n",
    "    \"\"\"Transforme les scores en probabilités (softmax).\"\"\"\n",
    "    scores = poids[lettre]\n",
    "    # L'exponentielle rend tous les scores positifs\n",
    "    exps = {b: math.exp(scores[b]) for b in scores}\n",
    "    total = sum(exps.values())\n",
    "    return {b: exps[b] / total for b in scores}\n",
    "\n",
    "# Au début, les probas sont quasi uniformes (le modèle devine au hasard)\n",
    "p = calculer_probas(poids, '.')\n",
    "lettres_debut = sorted(p.items(), key=lambda x: -x[1])[:5]\n",
    "print(\"Au début, le modèle pense que les prénoms commencent par :\")\n",
    "for lettre, prob in lettres_debut:\n",
    "    print(f\"  '{lettre}' : {prob:.1%}\")\n",
    "print(\"\\n  -> C'est n'importe quoi ! Il faut l'entraîner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3 : Entraînement\n",
    "\n",
    "L'algorithme est simple :\n",
    "1. Prendre un prénom d'entraînement\n",
    "2. Le modèle fait sa prédiction\n",
    "3. On calcule la loss (l'erreur)\n",
    "4. On **ajuste les poids** pour réduire la loss\n",
    "5. Recommencer\n",
    "\n",
    "L'étape 4 s'appelle la **descente de gradient**. C'est comme ajuster ton\n",
    "tir au panier un petit peu à chaque essai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prenoms = [\n",
    "    \"emma\", \"lucas\", \"lea\", \"hugo\", \"chloe\",\n",
    "    \"louis\", \"alice\", \"jules\", \"lina\", \"adam\",\n",
    "    \"rose\", \"arthur\", \"manon\", \"paul\", \"jade\",\n",
    "    \"nathan\", \"eva\", \"leo\", \"clara\", \"noah\",\n",
    "]\n",
    "\n",
    "# Vitesse d'apprentissage : de combien on ajuste à chaque fois\n",
    "# Trop grand = on dépasse, trop petit = on apprend trop lentement\n",
    "vitesse = 0.1\n",
    "\n",
    "print(\"Entraînement...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(50):\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "\n",
    "    for prenom in prenoms:\n",
    "        mot = \".\" + prenom + \".\"\n",
    "        for i in range(len(mot) - 1):\n",
    "            lettre = mot[i]\n",
    "            cible = mot[i + 1]\n",
    "\n",
    "            # 1. Prédiction\n",
    "            probas = calculer_probas(poids, lettre)\n",
    "\n",
    "            # 2. Loss\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "\n",
    "            # 3. Ajuster les poids (gradient simplifié)\n",
    "            for b in alphabet:\n",
    "                if b == cible:\n",
    "                    # La bonne réponse : augmenter son score\n",
    "                    poids[lettre][b] += vitesse * (1 - probas[b])\n",
    "                else:\n",
    "                    # Les mauvaises réponses : baisser leur score\n",
    "                    poids[lettre][b] -= vitesse * probas[b]\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"  Epoch {epoch:2d} | Loss moyenne : {loss_totale / nb:.3f}\")\n",
    "\n",
    "print(f\"  Epoch {epoch:2d} | Loss moyenne : {loss_totale / nb:.3f}\")\n",
    "print()\n",
    "print(\"La loss baisse = le modèle s'améliore !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voyons maintenant ce que le modèle a appris :\n",
    "p = calculer_probas(poids, '.')\n",
    "lettres_debut = sorted(p.items(), key=lambda x: -x[1])[:5]\n",
    "print(\"Après entraînement, les prénoms commencent par :\")\n",
    "for lettre, prob in lettres_debut:\n",
    "    print(f\"  '{lettre}' : {prob:.1%}\")\n",
    "print()\n",
    "print(\"C'est plus logique ! (l, a, e, c, j sont des débuts courants)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générons des prénoms avec le modèle entraîné\n",
    "def generer(poids, n=10):\n",
    "    resultats = []\n",
    "    for _ in range(n):\n",
    "        prenom = \"\"\n",
    "        lettre = \".\"\n",
    "        for _ in range(20):  # max 20 lettres\n",
    "            p = calculer_probas(poids, lettre)\n",
    "            choix = list(p.keys())\n",
    "            probs = list(p.values())\n",
    "            lettre = random.choices(choix, weights=probs, k=1)[0]\n",
    "            if lettre == \".\":\n",
    "                break\n",
    "            prenom += lettre\n",
    "        if prenom:\n",
    "            resultats.append(prenom.capitalize())\n",
    "    return resultats\n",
    "\n",
    "print(\"Prénoms inventés après entraînement :\")\n",
    "for p in generer(poids, 10):\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce qu'on a appris\n",
    "\n",
    "- La **loss** mesure à quel point le modèle se trompe\n",
    "- Les **poids** sont les nombres que le modèle ajuste pour apprendre\n",
    "- L'**entraînement** = ajuster les poids pour réduire la loss, encore et encore\n",
    "- Même un modèle simple s'améliore avec l'entraînement !\n",
    "\n",
    "### Limite\n",
    "\n",
    "Notre modèle ne regarde encore que **1 lettre en arrière**.\n",
    "Dans la prochaine leçon, on va lui donner une **mémoire** pour qu'il\n",
    "se souvienne de plusieurs lettres à la fois.\n",
    "\n",
    "---\n",
    "*Prochaine leçon : [03 - La mémoire du modèle](03_la_memoire_du_modele.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Sources (ISO 42001)\n\n- **Cross-entropy loss et descente de gradient** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, lignes implémentant le backward pass\n- **Analogie du gradient comme correction** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n- **Visualisation de la descente de gradient** : [3Blue1Brown - Gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w) — Grant Sanderson",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}