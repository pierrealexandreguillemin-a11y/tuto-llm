{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leçon 5 : Mon premier LLM\n",
    "\n",
    "## On assemble tout !\n",
    "\n",
    "Tu as appris :\n",
    "1. **Prédire la suite** avec des probabilités\n",
    "2. **Apprendre de ses erreurs** avec la loss et le gradient\n",
    "3. **Les embeddings** pour donner une mémoire au modèle\n",
    "4. **L'attention** pour regarder les lettres importantes\n",
    "\n",
    "Maintenant, on met tout ensemble pour créer un **vrai mini-LLM**\n",
    "qui génère des prénoms inventés !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture de notre LLM\n",
    "\n",
    "```\n",
    "Entrée : \"em\"  (on veut prédire 'm', 'a', '.')\n",
    "          |\n",
    "    [Token Embedding]   Chaque lettre -> vecteur de nombres\n",
    "          +\n",
    "    [Position Embedding] Chaque position -> vecteur de nombres\n",
    "          |\n",
    "    [Attention]          Chaque lettre regarde les précédentes\n",
    "          |\n",
    "    [MLP]                Réseau de neurones qui transforme l'info\n",
    "          |\n",
    "    [Softmax]            Transformer en probabilités\n",
    "          |\n",
    "    Sortie : probabilités pour chaque lettre\n",
    "```\n",
    "\n",
    "C'est la même architecture que GPT-2, GPT-3, GPT-4 !\n",
    "Juste en **beaucoup** plus petit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "VOCAB = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "VOCAB_SIZE = len(VOCAB)  # 27\n",
    "EMBED_DIM = 16  # taille des embeddings\n",
    "CONTEXT = 8  # nombre max de lettres en contexte\n",
    "NUM_HEADS = 1  # tête d'attention (simplifiée pour la clarté)\n",
    "HEAD_DIM = EMBED_DIM // NUM_HEADS  # 16\n",
    "HIDDEN_DIM = 32  # taille du MLP\n",
    "\n",
    "char_to_id = {c: i for i, c in enumerate(VOCAB)}\n",
    "id_to_char = {i: c for i, c in enumerate(VOCAB)}\n",
    "\n",
    "print(\"Configuration du mini-LLM :\")\n",
    "print(f\"  Vocabulaire : {VOCAB_SIZE} caractères\")\n",
    "print(f\"  Dimension embeddings : {EMBED_DIM}\")\n",
    "print(f\"  Contexte max : {CONTEXT} lettres\")\n",
    "print(f\"  Têtes d'attention : {NUM_HEADS}\")\n",
    "print(f\"  Taille MLP : {HIDDEN_DIM}\")\n",
    "\n",
    "# Comptons les paramètres\n",
    "nb_params = (\n",
    "    VOCAB_SIZE * EMBED_DIM  # token embeddings\n",
    "    + CONTEXT * EMBED_DIM  # position embeddings\n",
    "    + 3 * EMBED_DIM * EMBED_DIM  # Q, K, V pour attention\n",
    "    + EMBED_DIM * HIDDEN_DIM  # MLP couche 1\n",
    "    + HIDDEN_DIM * EMBED_DIM  # MLP couche 2\n",
    "    + EMBED_DIM * VOCAB_SIZE  # couche de sortie\n",
    ")\n",
    "print(f\"  Nombre de paramètres : ~{nb_params:,}\")\n",
    "print()\n",
    "print(f\"  (GPT-4 en a ~1,800,000,000,000 -- {nb_params / 1.8e12 * 100:.10f}% de GPT-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions utilitaires\n",
    "\n",
    "\n",
    "def rand_matrix(rows, cols, scale=0.3):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "def rand_vector(size, scale=0.3):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "\n",
    "def mat_vec(mat, vec):\n",
    "    \"\"\"Multiplication matrice x vecteur.\"\"\"\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "\n",
    "def vec_add(a, b):\n",
    "    return [x + y for x, y in zip(a, b, strict=False)]\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Si positif, on garde. Si négatif, on met à zéro.\"\"\"\n",
    "    return [max(0, v) for v in x]\n",
    "\n",
    "\n",
    "print(\"Fonctions utilitaires définies !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser tous les poids du modèle\n",
    "\n",
    "# Embeddings\n",
    "tok_emb = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.5)  # token -> vecteur\n",
    "pos_emb = rand_matrix(CONTEXT, EMBED_DIM, 0.5)  # position -> vecteur\n",
    "\n",
    "# Attention (1 tête pour la clarté)\n",
    "Wq = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wk = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wv = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "\n",
    "# MLP\n",
    "W1 = rand_matrix(HIDDEN_DIM, EMBED_DIM, 0.2)\n",
    "b1 = [0.0] * HIDDEN_DIM\n",
    "W2 = rand_matrix(EMBED_DIM, HIDDEN_DIM, 0.2)\n",
    "b2 = [0.0] * EMBED_DIM\n",
    "\n",
    "# Sortie\n",
    "W_out = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.2)\n",
    "\n",
    "print(\"Modèle initialisé avec des poids aléatoires.\")\n",
    "print(\"Il ne sait rien encore -- il faut l'entraîner !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_llm(sequence_ids):\n",
    "    \"\"\"Passe une séquence dans le mini-LLM et retourne les probas pour le prochain token.\"\"\"\n",
    "    n = len(sequence_ids)\n",
    "\n",
    "    # 1. Embeddings : token + position\n",
    "    hidden = []\n",
    "    for i, tok_id in enumerate(sequence_ids):\n",
    "        h = vec_add(tok_emb[tok_id], pos_emb[i % CONTEXT])\n",
    "        hidden.append(h)\n",
    "\n",
    "    # 2. Self-Attention (sur la dernière position)\n",
    "    # Query pour la dernière position\n",
    "    q = mat_vec(Wq, hidden[-1])\n",
    "\n",
    "    # Keys et Values pour toutes les positions\n",
    "    scores = []\n",
    "    values = []\n",
    "    for i in range(n):\n",
    "        k = mat_vec(Wk, hidden[i])\n",
    "        v = mat_vec(Wv, hidden[i])\n",
    "        score = sum(q[d] * k[d] for d in range(EMBED_DIM)) / math.sqrt(EMBED_DIM)\n",
    "        scores.append(score)\n",
    "        values.append(v)\n",
    "\n",
    "    attn_weights = softmax(scores)\n",
    "\n",
    "    # Somme pondérée des values\n",
    "    attn_out = [0.0] * EMBED_DIM\n",
    "    for i in range(n):\n",
    "        for d in range(EMBED_DIM):\n",
    "            attn_out[d] += attn_weights[i] * values[i][d]\n",
    "\n",
    "    # Connexion résiduelle\n",
    "    x = vec_add(hidden[-1], attn_out)\n",
    "\n",
    "    # 3. MLP\n",
    "    h = relu(vec_add(mat_vec(W1, x), b1))\n",
    "    mlp_out = vec_add(mat_vec(W2, h), b2)\n",
    "\n",
    "    # Connexion résiduelle\n",
    "    x = vec_add(x, mlp_out)\n",
    "\n",
    "    # 4. Sortie : scores pour chaque lettre\n",
    "    logits = mat_vec(W_out, x)\n",
    "    probas = softmax(logits)\n",
    "\n",
    "    return probas\n",
    "\n",
    "\n",
    "# Test avant entraînement\n",
    "test_ids = [char_to_id[c] for c in \".em\"]\n",
    "probas = forward_llm(test_ids)\n",
    "top5 = sorted(range(VOCAB_SIZE), key=lambda i: -probas[i])[:5]\n",
    "\n",
    "print(\"Avant entraînement -- prédictions après '.em' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"(La bonne réponse serait 'm' pour 'emma')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement simplifié\n",
    "# (On utilise une méthode numérique pour les gradients,\n",
    "#  plus lente mais plus facile à comprendre)\n",
    "\n",
    "prenoms = [\n",
    "    \"emma\",\n",
    "    \"lucas\",\n",
    "    \"lea\",\n",
    "    \"hugo\",\n",
    "    \"chloe\",\n",
    "    \"louis\",\n",
    "    \"alice\",\n",
    "    \"jules\",\n",
    "    \"lina\",\n",
    "    \"adam\",\n",
    "    \"rose\",\n",
    "    \"arthur\",\n",
    "    \"manon\",\n",
    "    \"paul\",\n",
    "    \"jade\",\n",
    "]\n",
    "\n",
    "\n",
    "def calcul_loss(prenoms):\n",
    "    \"\"\"Calcule la loss moyenne sur tous les prénoms.\"\"\"\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "    for prenom in prenoms:\n",
    "        mot = \".\" + prenom + \".\"\n",
    "        ids = [char_to_id[c] for c in mot]\n",
    "        for i in range(1, len(ids)):\n",
    "            seq = ids[:i]\n",
    "            cible = ids[i]\n",
    "            probas = forward_llm(seq[-CONTEXT:])\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "    return loss_totale / nb\n",
    "\n",
    "\n",
    "loss_initiale = calcul_loss(prenoms)\n",
    "print(f\"Loss initiale : {loss_initiale:.3f}\")\n",
    "print(f\"(Loss d'un modèle parfaitement aléatoire : {math.log(VOCAB_SIZE):.3f})\")\n",
    "print()\n",
    "print(\"L'entraînement complet prendrait du temps en Python pur.\")\n",
    "print(\"C'est pour ça qu'en vrai on utilise PyTorch avec des GPU !\")\n",
    "print()\n",
    "print(\"Mais l'ARCHITECTURE est exactement la même que GPT-2, GPT-3, GPT-4.\")\n",
    "print(\"Seuls la taille et la puissance de calcul changent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération (même sans entraînement complet, on peut voir le mécanisme)\n",
    "\n",
    "\n",
    "def generer_llm(debut=\".\", temperature=1.0, max_len=15):\n",
    "    \"\"\"Génère un prénom lettre par lettre avec notre mini-LLM.\"\"\"\n",
    "    ids = [char_to_id[c] for c in debut]\n",
    "    resultat = debut\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        probas = forward_llm(ids[-CONTEXT:])\n",
    "\n",
    "        # Température : < 1 = plus conservateur, > 1 = plus créatif\n",
    "        if temperature != 1.0:\n",
    "            logits = [math.log(p + 1e-10) / temperature for p in probas]\n",
    "            probas = softmax(logits)\n",
    "\n",
    "        # Choisir la prochaine lettre\n",
    "        idx = random.choices(range(VOCAB_SIZE), weights=probas, k=1)[0]\n",
    "\n",
    "        if idx == char_to_id[\".\"]:\n",
    "            break\n",
    "\n",
    "        ids.append(idx)\n",
    "        resultat += id_to_char[idx]\n",
    "\n",
    "    return resultat[1:] if resultat.startswith(\".\") else resultat\n",
    "\n",
    "\n",
    "print(\"Prénoms générés (modèle non-entraîné, juste pour montrer le mécanisme) :\")\n",
    "print()\n",
    "for _ in range(10):\n",
    "    p = generer_llm(temperature=0.8)\n",
    "    print(f\"  {p.capitalize()}\")\n",
    "\n",
    "print()\n",
    "print(\"C'est du charabia car le modèle n'est pas entraîné.\")\n",
    "print(\"Mais le MÉCANISME est exactement celui de ChatGPT !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récapitulatif : de 0 à GPT\n",
    "\n",
    "```\n",
    "Leçon 1 : Compter les lettres qui suivent        -> bigramme\n",
    "Leçon 2 : Apprendre de ses erreurs                -> entraînement\n",
    "Leçon 3 : Regarder plusieurs lettres en arrière   -> embeddings + contexte\n",
    "Leçon 4 : Choisir les lettres importantes          -> attention\n",
    "Leçon 5 : Assembler le tout                       -> mini-LLM !\n",
    "```\n",
    "\n",
    "## La différence avec ChatGPT\n",
    "\n",
    "| | Notre mini-LLM | ChatGPT |\n",
    "|---|---|---|\n",
    "| Architecture | La même ! | La même ! |\n",
    "| Paramètres | ~3,000 | ~1,800,000,000,000 |\n",
    "| Données | 15 prénoms | Internet entier |\n",
    "| Calcul | 1 PC, secondes | Des milliers de GPU, des mois |\n",
    "| Résultat | Prénoms inventés | Conversations, code, poésie... |\n",
    "\n",
    "L'algorithme est **le même**. La seule différence, c'est l'échelle.\n",
    "\n",
    "> *\"This file is the complete algorithm. Everything else is just efficiency.\"*\n",
    "> -- Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Pour aller plus loin\n\n- **microgpt.py** : Le code complet de Karpathy avec l'autograd et l'entraînement\n  [Lien](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)\n\n- **Vidéo \"Let's build GPT\"** : Karpathy explique tout en 2h\n  [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n\n- **nanoGPT** : Version avec PyTorch, entraînable pour de vrai\n  [GitHub](https://github.com/karpathy/nanoGPT)\n\n---\n\n**Félicitations ! Tu as compris comment fonctionne un LLM.**\n\n*Prochaine leçon : [06 - Entraîner le modèle](06_entrainer_le_modele.ipynb)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n### Sources (ISO 42001)\n\n- **Architecture complète GPT (embedding + attention + MLP + softmax)** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy\n- **Comparaison des paramètres GPT-4** : estimations publiques basées sur les rapports techniques OpenAI\n- **Explication du forward pass complet** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n- **Concept de température pour la génération** : même source, section sampling\n- **\"Attention Is All You Need\"** : Vaswani et al., 2017, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
