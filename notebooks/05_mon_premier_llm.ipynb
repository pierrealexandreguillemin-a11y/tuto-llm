{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecon 5 : Mon premier LLM\n",
    "\n",
    "## On assemble tout !\n",
    "\n",
    "Tu as appris :\n",
    "1. **Predire la suite** avec des probabilites\n",
    "2. **Apprendre de ses erreurs** avec la loss et le gradient\n",
    "3. **Les embeddings** pour donner une memoire au modele\n",
    "4. **L'attention** pour regarder les lettres importantes\n",
    "\n",
    "Maintenant, on met tout ensemble pour creer un **vrai mini-LLM**\n",
    "qui genere des prenoms inventes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture de notre LLM\n",
    "\n",
    "```\n",
    "Entree : \"em\"  (on veut predire 'm', 'a', '.')\n",
    "          |\n",
    "    [Token Embedding]   Chaque lettre -> vecteur de nombres\n",
    "          +\n",
    "    [Position Embedding] Chaque position -> vecteur de nombres\n",
    "          |\n",
    "    [Attention]          Chaque lettre regarde les precedentes\n",
    "          |\n",
    "    [MLP]                Reseau de neurones qui transforme l'info\n",
    "          |\n",
    "    [Softmax]            Transformer en probabilites\n",
    "          |\n",
    "    Sortie : probabilites pour chaque lettre\n",
    "```\n",
    "\n",
    "C'est la meme architecture que GPT-2, GPT-3, GPT-4 !\n",
    "Juste en **beaucoup** plus petit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "VOCAB = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "VOCAB_SIZE = len(VOCAB)  # 27\n",
    "EMBED_DIM = 16           # taille des embeddings\n",
    "CONTEXT = 8              # nombre max de lettres en contexte\n",
    "NUM_HEADS = 2            # tetes d'attention\n",
    "HEAD_DIM = EMBED_DIM // NUM_HEADS  # 8\n",
    "HIDDEN_DIM = 32          # taille du MLP\n",
    "\n",
    "char_to_id = {c: i for i, c in enumerate(VOCAB)}\n",
    "id_to_char = {i: c for i, c in enumerate(VOCAB)}\n",
    "\n",
    "print(\"Configuration du mini-LLM :\")\n",
    "print(f\"  Vocabulaire : {VOCAB_SIZE} caracteres\")\n",
    "print(f\"  Dimension embeddings : {EMBED_DIM}\")\n",
    "print(f\"  Contexte max : {CONTEXT} lettres\")\n",
    "print(f\"  Tetes d'attention : {NUM_HEADS}\")\n",
    "print(f\"  Taille MLP : {HIDDEN_DIM}\")\n",
    "\n",
    "# Comptons les parametres\n",
    "nb_params = (\n",
    "    VOCAB_SIZE * EMBED_DIM +     # token embeddings\n",
    "    CONTEXT * EMBED_DIM +         # position embeddings\n",
    "    4 * EMBED_DIM * EMBED_DIM +   # Q, K, V, output pour attention\n",
    "    EMBED_DIM * HIDDEN_DIM +      # MLP couche 1\n",
    "    HIDDEN_DIM * EMBED_DIM +      # MLP couche 2\n",
    "    EMBED_DIM * VOCAB_SIZE         # couche de sortie\n",
    ")\n",
    "print(f\"  Nombre de parametres : ~{nb_params:,}\")\n",
    "print()\n",
    "print(f\"  (GPT-4 en a ~1,800,000,000,000 -- {nb_params / 1.8e12 * 100:.10f}% de GPT-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions utilitaires\n",
    "\n",
    "def rand_matrix(rows, cols, scale=0.3):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "def rand_vector(size, scale=0.3):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "def mat_vec(mat, vec):\n",
    "    \"\"\"Multiplication matrice x vecteur.\"\"\"\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "def vec_add(a, b):\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "def softmax(scores):\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Si positif, on garde. Si negatif, on met a zero.\"\"\"\n",
    "    return [max(0, v) for v in x]\n",
    "\n",
    "print(\"Fonctions utilitaires definies !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser tous les poids du modele\n",
    "\n",
    "# Embeddings\n",
    "tok_emb = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.5)  # token -> vecteur\n",
    "pos_emb = rand_matrix(CONTEXT, EMBED_DIM, 0.5)      # position -> vecteur\n",
    "\n",
    "# Attention (simplifiee : 1 tete pour la clarte)\n",
    "Wq = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wk = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wv = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "\n",
    "# MLP\n",
    "W1 = rand_matrix(HIDDEN_DIM, EMBED_DIM, 0.2)\n",
    "b1 = [0.0] * HIDDEN_DIM\n",
    "W2 = rand_matrix(EMBED_DIM, HIDDEN_DIM, 0.2)\n",
    "b2 = [0.0] * EMBED_DIM\n",
    "\n",
    "# Sortie\n",
    "W_out = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.2)\n",
    "\n",
    "print(\"Modele initialise avec des poids aleatoires.\")\n",
    "print(\"Il ne sait rien encore -- il faut l'entrainer !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_llm(sequence_ids):\n",
    "    \"\"\"Passe une sequence dans le mini-LLM et retourne les probas pour le prochain token.\"\"\"\n",
    "    n = len(sequence_ids)\n",
    "\n",
    "    # 1. Embeddings : token + position\n",
    "    hidden = []\n",
    "    for i, tok_id in enumerate(sequence_ids):\n",
    "        h = vec_add(tok_emb[tok_id], pos_emb[i % CONTEXT])\n",
    "        hidden.append(h)\n",
    "\n",
    "    # 2. Self-Attention (sur la derniere position)\n",
    "    # Query pour la derniere position\n",
    "    q = mat_vec(Wq, hidden[-1])\n",
    "\n",
    "    # Keys et Values pour toutes les positions\n",
    "    scores = []\n",
    "    values = []\n",
    "    for i in range(n):\n",
    "        k = mat_vec(Wk, hidden[i])\n",
    "        v = mat_vec(Wv, hidden[i])\n",
    "        score = sum(q[d] * k[d] for d in range(EMBED_DIM)) / math.sqrt(EMBED_DIM)\n",
    "        scores.append(score)\n",
    "        values.append(v)\n",
    "\n",
    "    attn_weights = softmax(scores)\n",
    "\n",
    "    # Somme ponderee des values\n",
    "    attn_out = [0.0] * EMBED_DIM\n",
    "    for i in range(n):\n",
    "        for d in range(EMBED_DIM):\n",
    "            attn_out[d] += attn_weights[i] * values[i][d]\n",
    "\n",
    "    # Connexion residuelle\n",
    "    x = vec_add(hidden[-1], attn_out)\n",
    "\n",
    "    # 3. MLP\n",
    "    h = relu(vec_add(mat_vec(W1, x), b1))\n",
    "    mlp_out = vec_add(mat_vec(W2, h), b2)\n",
    "\n",
    "    # Connexion residuelle\n",
    "    x = vec_add(x, mlp_out)\n",
    "\n",
    "    # 4. Sortie : scores pour chaque lettre\n",
    "    logits = mat_vec(W_out, x)\n",
    "    probas = softmax(logits)\n",
    "\n",
    "    return probas\n",
    "\n",
    "# Test avant entrainement\n",
    "test_ids = [char_to_id[c] for c in \".em\"]\n",
    "probas = forward_llm(test_ids)\n",
    "top5 = sorted(range(VOCAB_SIZE), key=lambda i: -probas[i])[:5]\n",
    "\n",
    "print(\"Avant entrainement -- predictions apres '.em' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"(La bonne reponse serait 'm' pour 'emma')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement simplifie\n",
    "# (On utilise une methode numerique pour les gradients,\n",
    "#  plus lente mais plus facile a comprendre)\n",
    "\n",
    "prenoms = [\n",
    "    \"emma\", \"lucas\", \"lea\", \"hugo\", \"chloe\",\n",
    "    \"louis\", \"alice\", \"jules\", \"lina\", \"adam\",\n",
    "    \"rose\", \"arthur\", \"manon\", \"paul\", \"jade\",\n",
    "]\n",
    "\n",
    "def calcul_loss(prenoms):\n",
    "    \"\"\"Calcule la loss moyenne sur tous les prenoms.\"\"\"\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "    for prenom in prenoms:\n",
    "        mot = \".\" + prenom + \".\"\n",
    "        ids = [char_to_id[c] for c in mot]\n",
    "        for i in range(1, len(ids)):\n",
    "            seq = ids[:i]\n",
    "            cible = ids[i]\n",
    "            probas = forward_llm(seq[-CONTEXT:])\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "    return loss_totale / nb\n",
    "\n",
    "loss_initiale = calcul_loss(prenoms)\n",
    "print(f\"Loss initiale : {loss_initiale:.3f}\")\n",
    "print(f\"(Loss d'un modele parfaitement aleatoire : {math.log(VOCAB_SIZE):.3f})\")\n",
    "print()\n",
    "print(\"L'entrainement complet prendrait du temps en Python pur.\")\n",
    "print(\"C'est pour ca qu'en vrai on utilise PyTorch avec des GPU !\")\n",
    "print()\n",
    "print(\"Mais l'ARCHITECTURE est exactement la meme que GPT-2, GPT-3, GPT-4.\")\n",
    "print(\"Seuls la taille et la puissance de calcul changent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation (meme sans entrainement complet, on peut voir le mecanisme)\n",
    "\n",
    "def generer_llm(debut=\".\", temperature=1.0, max_len=15):\n",
    "    \"\"\"Genere un prenom lettre par lettre avec notre mini-LLM.\"\"\"\n",
    "    ids = [char_to_id[c] for c in debut]\n",
    "    resultat = debut\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        probas = forward_llm(ids[-CONTEXT:])\n",
    "\n",
    "        # Temperature : < 1 = plus conservateur, > 1 = plus creatif\n",
    "        if temperature != 1.0:\n",
    "            logits = [math.log(p + 1e-10) / temperature for p in probas]\n",
    "            probas = softmax(logits)\n",
    "\n",
    "        # Choisir la prochaine lettre\n",
    "        idx = random.choices(range(VOCAB_SIZE), weights=probas, k=1)[0]\n",
    "\n",
    "        if idx == char_to_id['.']:\n",
    "            break\n",
    "\n",
    "        ids.append(idx)\n",
    "        resultat += id_to_char[idx]\n",
    "\n",
    "    return resultat[1:] if resultat.startswith('.') else resultat\n",
    "\n",
    "print(\"Prenoms generes (modele non-entraine, juste pour montrer le mecanisme) :\")\n",
    "print()\n",
    "for _ in range(10):\n",
    "    p = generer_llm(temperature=0.8)\n",
    "    print(f\"  {p.capitalize()}\")\n",
    "\n",
    "print()\n",
    "print(\"C'est du charabia car le modele n'est pas entraine.\")\n",
    "print(\"Mais le MECANISME est exactement celui de ChatGPT !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recapitulatif : de 0 a GPT\n",
    "\n",
    "```\n",
    "Lecon 1 : Compter les lettres qui suivent        -> bigramme\n",
    "Lecon 2 : Apprendre de ses erreurs                -> entrainement\n",
    "Lecon 3 : Regarder plusieurs lettres en arriere   -> embeddings + contexte\n",
    "Lecon 4 : Choisir les lettres importantes          -> attention\n",
    "Lecon 5 : Assembler le tout                       -> mini-LLM !\n",
    "```\n",
    "\n",
    "## La difference avec ChatGPT\n",
    "\n",
    "| | Notre mini-LLM | ChatGPT |\n",
    "|---|---|---|\n",
    "| Architecture | La meme ! | La meme ! |\n",
    "| Parametres | ~3,000 | ~1,800,000,000,000 |\n",
    "| Donnees | 15 prenoms | Internet entier |\n",
    "| Calcul | 1 PC, secondes | Des milliers de GPU, des mois |\n",
    "| Resultat | Prenoms inventes | Conversations, code, poesie... |\n",
    "\n",
    "L'algorithme est **le meme**. La seule difference, c'est l'echelle.\n",
    "\n",
    "> *\"This file is the complete algorithm. Everything else is just efficiency.\"*\n",
    "> -- Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "- **microgpt.py** : Le code complet de Karpathy avec l'autograd et l'entrainement\n",
    "  [Lien](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)\n",
    "\n",
    "- **Video \"Let's build GPT\"** : Karpathy explique tout en 2h\n",
    "  [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "\n",
    "- **nanoGPT** : Version avec PyTorch, entrainable pour de vrai\n",
    "  [GitHub](https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "---\n",
    "\n",
    "**Felicitations ! Tu as compris comment fonctionne un LLM.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
