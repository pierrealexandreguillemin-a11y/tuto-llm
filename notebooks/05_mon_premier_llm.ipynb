{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Rappel** : clique sur une cellule grise, puis **Shift + Entree** pour l'executer.\n> Execute les cellules **dans l'ordre** de haut en bas.\n\n---\n\n# Leçon 5 : Mon premier LLM\n\n## On assemble tout !\n\nTu as appris :\n1. **Prédire la suite** avec des probabilités\n2. **Apprendre de ses erreurs** avec la loss et le gradient\n3. **Les embeddings** pour donner une mémoire au modèle\n4. **L'attention** pour regarder les lettres importantes\n\nMaintenant, on met tout ensemble pour créer un **vrai mini-LLM**\nqui génère des noms de Pokémon inventés !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "_exercices_faits = set()\n",
    "_NB_TOTAL = 3\n",
    "\n",
    "\n",
    "def verifier(num_exercice, condition, message_ok, message_aide=\"\"):\n",
    "    \"\"\"Valide un exercice avec feedback HTML vert/rouge + compteur.\"\"\"\n",
    "    try:\n",
    "        _result = bool(condition)\n",
    "    except Exception:\n",
    "        _result = False\n",
    "    if _result:\n",
    "        _exercices_faits.add(num_exercice)\n",
    "        n = len(_exercices_faits)\n",
    "        barre = \"\\U0001f7e9\" * n + \"\\u2b1c\" * (_NB_TOTAL - n)\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<div style=\"padding:10px;background:#d4edda;border-left:5px solid #28a745;'\n",
    "                f'margin:8px 0;border-radius:4px;font-family:system-ui,-apple-system,sans-serif\">'\n",
    "                f\"\\u2705 <b>{message_ok}</b><br>\"\n",
    "                f'<span style=\"color:#555\">Progression : {barre} {n}/{_NB_TOTAL}</span></div>'\n",
    "            )\n",
    "        )\n",
    "        if n == _NB_TOTAL:\n",
    "            display(\n",
    "                HTML(\n",
    "                    '<div style=\"padding:12px;background:linear-gradient(135deg,#3949ab,#6a1b9a);'\n",
    "                    \"color:white;border-radius:8px;text-align:center;font-family:system-ui,-apple-system,sans-serif;\"\n",
    "                    'font-size:1.2em;margin:8px 0\">\\U0001f3c6 <b>Bravo ! Toutes les activites de cette lecon sont terminees !</b></div>'\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<div style=\"padding:10px;background:#fff3cd;border-left:5px solid #ffc107;'\n",
    "                f'margin:8px 0;border-radius:4px;font-family:system-ui,-apple-system,sans-serif\">'\n",
    "                f\"\\U0001f4a1 <b>{message_aide}</b></div>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def exercice(numero, titre, consigne, observation=\"\"):\n",
    "    \"\"\"Affiche la banniere d'exercice.\"\"\"\n",
    "\n",
    "    def _style_code(text):\n",
    "        return text.replace(\n",
    "            \"<code>\",\n",
    "            '<code style=\"font-size:0.95em;background:#bbdefb;'\n",
    "            'padding:1px 5px;border-radius:3px;font-family:monospace;\">',\n",
    "        )\n",
    "\n",
    "    obs = \"\"\n",
    "    if observation:\n",
    "        obs = (\n",
    "            f'<div style=\"margin-top:6px;color:#555;font-size:0.92em;\">'\n",
    "            f\"<b>Ce que tu vas voir\\u00a0:</b> {_style_code(observation)}</div>\"\n",
    "        )\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<div style=\"border-left:5px solid #1565c0;background:#e8f0fe;'\n",
    "            f\"padding:12px 16px; margin:4px 0 10px 0; border-radius:0 8px 8px 0;\"\n",
    "            f'font-family:system-ui,-apple-system,sans-serif; font-size:0.95em;\">'\n",
    "            f'<b style=\"color:#0d47a1;\">Exercice\\u00a0{numero} \\u2014 {titre}</b><br>'\n",
    "            f\"{_style_code(consigne)}{obs}</div>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def afficher_barres(valeurs, etiquettes, titre=\"Probabilites\"):\n",
    "    \"\"\"Affiche des barres horizontales HTML.\"\"\"\n",
    "    rows = \"\"\n",
    "    max_val = max(valeurs) if valeurs else 1\n",
    "    for etiq, val in zip(etiquettes, valeurs, strict=False):\n",
    "        pct = val / max_val * 100 if max_val > 0 else 0\n",
    "        rows += (\n",
    "            f'<tr><td style=\"padding:3px 8px;font-weight:bold;font-size:1em\">{etiq}</td>'\n",
    "            f'<td style=\"padding:3px;width:300px\"><div style=\"background:linear-gradient(90deg,#3949ab,#6a1b9a);'\n",
    "            f'width:{max(pct, 2):.0f}%;height:20px;border-radius:4px\"></div></td>'\n",
    "            f'<td style=\"padding:3px 8px;font-size:0.9em\">{val:.0%}</td></tr>'\n",
    "        )\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<!-- tuto-viz --><div style=\"margin:8px 0\"><b>{titre}</b>'\n",
    "            f'<table style=\"border-collapse:collapse;margin-top:4px\">{rows}</table></div>'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def afficher_attention(poids, positions, titre=\"Poids d'attention\"):\n",
    "    \"\"\"Affiche les poids d'attention sous forme de barres horizontales.\"\"\"\n",
    "    rows = \"\"\n",
    "    max_val = max(poids) if poids else 1\n",
    "    for pos, w in zip(positions, poids, strict=False):\n",
    "        pct = w / max_val * 100 if max_val > 0 else 0\n",
    "        rows += (\n",
    "            f'<tr><td style=\"padding:3px 8px;font-weight:bold;font-size:1em\">{pos}</td>'\n",
    "            f'<td style=\"padding:3px;width:300px\"><div style=\"background:linear-gradient(90deg,#3949ab,#6a1b9a);'\n",
    "            f'width:{max(pct, 2):.0f}%;height:20px;border-radius:4px\"></div></td>'\n",
    "            f'<td style=\"padding:3px 8px;font-size:0.9em\">{w:.1%}</td></tr>'\n",
    "        )\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<!-- tuto-viz --><div style=\"margin:8px 0\"><b>{titre}</b>'\n",
    "            f'<table style=\"border-collapse:collapse;margin-top:4px\">{rows}</table></div>'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def afficher_architecture():\n",
    "    \"\"\"Affiche le schema de reseau du mini-LLM avec connexions residuelles.\"\"\"\n",
    "    # Styles communs\n",
    "    box = (\n",
    "        \"padding:8px 12px;border-radius:8px;text-align:center;\"\n",
    "        \"font-family:system-ui,-apple-system,sans-serif;position:relative;z-index:1\"\n",
    "    )\n",
    "    dim = \"font-size:0.75em;color:#888;font-style:italic\"\n",
    "    arrow = \"font-size:1.2em;color:#999\"\n",
    "\n",
    "    html = (\n",
    "        '<!-- tuto-viz --><div style=\"margin:8px 0\"><b>Architecture du mini-LLM</b>'\n",
    "        '<div style=\"display:flex;flex-direction:column;align-items:center;'\n",
    "        'gap:2px;margin-top:8px;position:relative\">'\n",
    "        # Entree\n",
    "        f'<div style=\"background:#f8f9fa;border:2px solid #495057;{box};width:320px\">'\n",
    "        '<b style=\"color:#495057\">\\U0001f4e5 Entree</b><br>'\n",
    "        f'<span style=\"{dim}\">\".pik\" \\u2192 [0, 16, 9, 11]</span></div>'\n",
    "        f'<div style=\"{arrow}\">\\u25bc</div>'\n",
    "        # Embeddings\n",
    "        f'<div style=\"background:#e8f5e9;border:2px solid #2e7d32;{box};width:320px\">'\n",
    "        '<b style=\"color:#2e7d32\">\\U0001f3af Token + Position Embedding</b><br>'\n",
    "        f'<span style=\"{dim}\">tok_emb[id] + pos_emb[pos] \\u2192 [n, {\"{EMBED_DIM}\"}]</span></div>'\n",
    "        f'<div style=\"{arrow}\">\\u25bc</div>'\n",
    "    )\n",
    "\n",
    "    # Self-Attention block avec detail Q/K/V + skip connection\n",
    "    html += (\n",
    "        '<div style=\"position:relative;width:380px\">'\n",
    "        # Skip connection gauche (ligne verticale qui contourne l'attention)\n",
    "        '<div style=\"position:absolute;left:0;top:0;bottom:0;width:30px\">'\n",
    "        '<div style=\"border-left:2px dashed #1565c0;border-top:2px dashed #1565c0;'\n",
    "        \"border-bottom:2px dashed #1565c0;height:100%;margin-left:10px;\"\n",
    "        'border-radius:8px 0 0 8px\"></div></div>'\n",
    "        '<div style=\"position:absolute;left:2px;top:50%;transform:translateY(-50%) rotate(90deg);'\n",
    "        f'font-size:0.65em;color:#1565c0;white-space:nowrap\">\\u2795 residuelle</div>'\n",
    "        # Boite attention\n",
    "        '<div style=\"margin-left:35px;margin-right:5px\">'\n",
    "        f'<div style=\"background:#e3f2fd;border:2px solid #1565c0;{box}\">'\n",
    "        '<b style=\"color:#1565c0\">\\U0001f50d Self-Attention</b><br>'\n",
    "        f'<span style=\"{dim}\">Q = Wq \\u00d7 x, K = Wk \\u00d7 x, V = Wv \\u00d7 x<br>'\n",
    "        \"scores = Q \\u00b7 K / \\u221ad \\u2192 softmax \\u2192 \\u2211 poids \\u00d7 V</span>\"\n",
    "        \"</div></div></div>\"\n",
    "        f'<div style=\"{arrow}\">\\u25bc + skip</div>'\n",
    "    )\n",
    "\n",
    "    # MLP block avec skip connection\n",
    "    html += (\n",
    "        '<div style=\"position:relative;width:380px\">'\n",
    "        # Skip connection gauche (ligne verticale qui contourne le MLP)\n",
    "        '<div style=\"position:absolute;left:0;top:0;bottom:0;width:30px\">'\n",
    "        '<div style=\"border-left:2px dashed #e65100;border-top:2px dashed #e65100;'\n",
    "        \"border-bottom:2px dashed #e65100;height:100%;margin-left:10px;\"\n",
    "        'border-radius:8px 0 0 8px\"></div></div>'\n",
    "        '<div style=\"position:absolute;left:2px;top:50%;transform:translateY(-50%) rotate(90deg);'\n",
    "        f'font-size:0.65em;color:#e65100;white-space:nowrap\">\\u2795 residuelle</div>'\n",
    "        # Boite MLP\n",
    "        '<div style=\"margin-left:35px;margin-right:5px\">'\n",
    "        f'<div style=\"background:#fff3e0;border:2px solid #e65100;{box}\">'\n",
    "        '<b style=\"color:#e65100\">\\U0001f9e0 MLP (ReLU)</b><br>'\n",
    "        f'<span style=\"{dim}\">W1 \\u00d7 x + b1 \\u2192 ReLU \\u2192 W2 \\u00d7 h + b2</span>'\n",
    "        \"</div></div></div>\"\n",
    "        f'<div style=\"{arrow}\">\\u25bc + skip</div>'\n",
    "    )\n",
    "\n",
    "    # Projection + Softmax + Sortie\n",
    "    html += (\n",
    "        f'<div style=\"background:#fce4ec;border:2px solid #c62828;{box};width:320px\">'\n",
    "        '<b style=\"color:#c62828\">\\U0001f4ca Projection + Softmax</b><br>'\n",
    "        f'<span style=\"{dim}\">W_out \\u00d7 x \\u2192 logits \\u2192 softmax \\u2192 probas</span></div>'\n",
    "        f'<div style=\"{arrow}\">\\u25bc</div>'\n",
    "        f'<div style=\"background:#f3e5f5;border:2px solid #6a1b9a;{box};width:320px\">'\n",
    "        '<b style=\"color:#6a1b9a\">\\U0001f4e4 Sortie</b><br>'\n",
    "        f'<span style=\"{dim}\">Probabilite pour chaque lettre (27 valeurs)</span></div>'\n",
    "        \"</div>\"\n",
    "        '<div style=\"margin-top:8px;color:#555;font-size:0.92em;text-align:center\">'\n",
    "        \"C'est la meme architecture que GPT-2/3/4, juste en beaucoup plus petit.<br>\"\n",
    "        '<span style=\"color:#1565c0\">Les lignes pointillees</span> sont les '\n",
    "        \"<b>connexions residuelles</b> (raccourcis qui aident l'apprentissage).\"\n",
    "        \"</div></div>\"\n",
    "    )\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "print(\"Outils de visualisation charges !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture de notre LLM\n",
    "\n",
    "Execute la cellule ci-dessous pour voir l'architecture de notre mini-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "VOCAB = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "VOCAB_SIZE = len(VOCAB)  # 27\n",
    "EMBED_DIM = 16  # taille des embeddings\n",
    "CONTEXT = 8  # nombre max de lettres en contexte\n",
    "NUM_HEADS = 1  # tête d'attention (simplifiée pour la clarté)\n",
    "HEAD_DIM = EMBED_DIM // NUM_HEADS  # = 16 (identique à EMBED_DIM avec 1 tête)\n",
    "HIDDEN_DIM = 32  # taille du MLP\n",
    "\n",
    "char_to_id = {c: i for i, c in enumerate(VOCAB)}\n",
    "id_to_char = {i: c for i, c in enumerate(VOCAB)}\n",
    "\n",
    "print(\"Configuration du mini-LLM :\")\n",
    "print(f\"  Vocabulaire : {VOCAB_SIZE} caractères\")\n",
    "print(f\"  Dimension embeddings : {EMBED_DIM}\")\n",
    "print(f\"  Contexte max : {CONTEXT} lettres\")\n",
    "print(f\"  Têtes d'attention : {NUM_HEADS}\")\n",
    "print(f\"  Dimension par tête : {HEAD_DIM}\")\n",
    "print(f\"  Taille MLP : {HIDDEN_DIM}\")\n",
    "\n",
    "# Comptons les paramètres\n",
    "nb_params = (\n",
    "    VOCAB_SIZE * EMBED_DIM  # token embeddings       = 27*16 = 432\n",
    "    + CONTEXT * EMBED_DIM  # position embeddings    = 8*16  = 128\n",
    "    + 3 * EMBED_DIM * EMBED_DIM  # Q, K, V              = 3*256 = 768\n",
    "    + EMBED_DIM * HIDDEN_DIM  # MLP couche 1 (poids)  = 16*32 = 512\n",
    "    + HIDDEN_DIM  # MLP couche 1 (biais)  = 32\n",
    "    + HIDDEN_DIM * EMBED_DIM  # MLP couche 2 (poids)  = 32*16 = 512\n",
    "    + EMBED_DIM  # MLP couche 2 (biais)  = 16\n",
    "    + EMBED_DIM * VOCAB_SIZE  # couche de sortie      = 16*27 = 432\n",
    ")\n",
    "print(f\"  Nombre de paramètres : {nb_params:,}\")\n",
    "print()\n",
    "print(f\"  (GPT-4 en a ~1,800,000,000,000 -- {nb_params / 1.8e12 * 100:.10f}% de GPT-4)\")\n",
    "\n",
    "# Visualisation de l'architecture\n",
    "afficher_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercice(\n",
    "    1,\n",
    "    \"Change la taille du modele\",\n",
    "    \"Change <code>EMBED_DIM_test</code> ci-dessous (essaie 8 ou 32), puis <b>Shift + Entree</b>.\",\n",
    "    \"Doubler EMBED_DIM quadruple presque le nombre de parametres !\",\n",
    ")\n",
    "\n",
    "# ==== MODIFIE ICI ====\n",
    "EMBED_DIM_test = 16  # <-- Essaie 8 (petit) ou 32 (grand) !\n",
    "# ======================\n",
    "\n",
    "HIDDEN_DIM_test = EMBED_DIM_test * 2\n",
    "\n",
    "nb_params_test = (\n",
    "    VOCAB_SIZE * EMBED_DIM_test  # token embeddings\n",
    "    + CONTEXT * EMBED_DIM_test  # position embeddings\n",
    "    + 3 * EMBED_DIM_test * EMBED_DIM_test  # Q, K, V\n",
    "    + EMBED_DIM_test * HIDDEN_DIM_test  # MLP couche 1 (poids)\n",
    "    + HIDDEN_DIM_test  # MLP couche 1 (biais)\n",
    "    + HIDDEN_DIM_test * EMBED_DIM_test  # MLP couche 2 (poids)\n",
    "    + EMBED_DIM_test  # MLP couche 2 (biais)\n",
    "    + EMBED_DIM_test * VOCAB_SIZE  # sortie\n",
    ")\n",
    "\n",
    "print(f\"Avec EMBED_DIM = {EMBED_DIM_test} :\")\n",
    "print(f\"  Nombre de parametres : {nb_params_test:,}\")\n",
    "print(f\"  (Notre modele en a {nb_params:,})\")\n",
    "if EMBED_DIM_test < EMBED_DIM:\n",
    "    print(\"  -> Plus petit : moins de parametres, plus rapide, mais moins precis.\")\n",
    "elif EMBED_DIM_test > EMBED_DIM:\n",
    "    print(\n",
    "        \"  -> Plus grand : plus de parametres, plus lent, mais potentiellement meilleur.\"\n",
    "    )\n",
    "\n",
    "# Validation exercice 1\n",
    "verifier(\n",
    "    1,\n",
    "    EMBED_DIM_test != 16,\n",
    "    f\"Bien joue ! Avec {EMBED_DIM_test} dimensions, le modele a {nb_params_test:,} parametres.\",\n",
    "    \"Change EMBED_DIM_test pour une autre valeur, par exemple 8 ou 32.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions utilitaires\n",
    "\n",
    "\n",
    "def rand_matrix(rows, cols, scale=0.3):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "def rand_vector(size, scale=0.3):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "\n",
    "def mat_vec(mat, vec):\n",
    "    \"\"\"Multiplication matrice x vecteur.\"\"\"\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "\n",
    "def vec_add(a, b):\n",
    "    return [x + y for x, y in zip(a, b, strict=False)]\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Si positif, on garde. Si négatif, on met à zéro.\"\"\"\n",
    "    return [max(0, v) for v in x]\n",
    "\n",
    "\n",
    "print(\"Fonctions utilitaires définies !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser tous les poids du modèle\n",
    "\n",
    "# Embeddings\n",
    "tok_emb = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.5)  # token -> vecteur\n",
    "pos_emb = rand_matrix(CONTEXT, EMBED_DIM, 0.5)  # position -> vecteur\n",
    "\n",
    "# Attention (1 tête pour la clarté)\n",
    "Wq = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wk = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wv = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "\n",
    "# MLP\n",
    "W1 = rand_matrix(HIDDEN_DIM, EMBED_DIM, 0.2)\n",
    "b1 = rand_vector(HIDDEN_DIM, 0.1)\n",
    "W2 = rand_matrix(EMBED_DIM, HIDDEN_DIM, 0.2)\n",
    "b2 = rand_vector(EMBED_DIM, 0.1)\n",
    "\n",
    "# Sortie\n",
    "W_out = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.2)\n",
    "\n",
    "print(\"Modèle initialisé avec des poids aléatoires.\")\n",
    "print(\"Il ne sait rien encore -- il faut l'entraîner !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_llm(sequence_ids):\n",
    "    \"\"\"Passe une séquence dans le mini-LLM et retourne les probas pour le prochain token.\"\"\"\n",
    "    n = len(sequence_ids)\n",
    "\n",
    "    # 1. Embeddings : token + position\n",
    "    hidden = []\n",
    "    for i, tok_id in enumerate(sequence_ids):\n",
    "        h = vec_add(tok_emb[tok_id], pos_emb[i % CONTEXT])\n",
    "        hidden.append(h)\n",
    "\n",
    "    # 2. Self-Attention (sur la dernière position)\n",
    "    # Query pour la dernière position\n",
    "    q = mat_vec(Wq, hidden[-1])\n",
    "\n",
    "    # Keys et Values pour toutes les positions\n",
    "    scores = []\n",
    "    values = []\n",
    "    for i in range(n):\n",
    "        k = mat_vec(Wk, hidden[i])\n",
    "        v = mat_vec(Wv, hidden[i])\n",
    "        score = sum(q[d] * k[d] for d in range(EMBED_DIM)) / math.sqrt(EMBED_DIM)\n",
    "        scores.append(score)\n",
    "        values.append(v)\n",
    "\n",
    "    attn_weights = softmax(scores)\n",
    "\n",
    "    # Somme pondérée des values\n",
    "    attn_out = [0.0] * EMBED_DIM\n",
    "    for i in range(n):\n",
    "        for d in range(EMBED_DIM):\n",
    "            attn_out[d] += attn_weights[i] * values[i][d]\n",
    "\n",
    "    # Connexion résiduelle\n",
    "    x = vec_add(hidden[-1], attn_out)\n",
    "\n",
    "    # 3. MLP\n",
    "    h = relu(vec_add(mat_vec(W1, x), b1))\n",
    "    mlp_out = vec_add(mat_vec(W2, h), b2)\n",
    "\n",
    "    # Connexion résiduelle\n",
    "    x = vec_add(x, mlp_out)\n",
    "\n",
    "    # 4. Sortie : scores pour chaque lettre\n",
    "    logits = mat_vec(W_out, x)\n",
    "    probas = softmax(logits)\n",
    "\n",
    "    return probas\n",
    "\n",
    "\n",
    "def _calculer_poids_attention(texte):\n",
    "    \"\"\"Calcule les poids d'attention pour un texte (utilise les poids globaux).\"\"\"\n",
    "    ids = [char_to_id[c] for c in texte]\n",
    "    hidden = [vec_add(tok_emb[tid], pos_emb[i % CONTEXT]) for i, tid in enumerate(ids)]\n",
    "    q = mat_vec(Wq, hidden[-1])\n",
    "    scores = []\n",
    "    for i in range(len(ids)):\n",
    "        k = mat_vec(Wk, hidden[i])\n",
    "        score = sum(q[d] * k[d] for d in range(EMBED_DIM)) / math.sqrt(EMBED_DIM)\n",
    "        scores.append(score)\n",
    "    return softmax(scores)\n",
    "\n",
    "\n",
    "# Test avant entraînement\n",
    "test_ids = [char_to_id[c] for c in \".pik\"]\n",
    "probas = forward_llm(test_ids)\n",
    "top5 = sorted(range(VOCAB_SIZE), key=lambda i: -probas[i])[:5]\n",
    "\n",
    "print(\"Avant entraînement -- prédictions après '.pik' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print(\"(La bonne réponse serait 'a' pour 'pikachu')\")\n",
    "\n",
    "# Visualisation des top-5 predictions\n",
    "afficher_barres(\n",
    "    [probas[i] for i in top5],\n",
    "    [id_to_char[i] for i in top5],\n",
    "    titre=\"Top 5 predictions apres '.pik' (avant entrainement)\",\n",
    ")\n",
    "# Visualisation des poids d'attention\n",
    "_attn_viz = _calculer_poids_attention(\".pik\")\n",
    "afficher_attention(_attn_viz, list(\".pik\"), titre=\"Poids d'attention pour '.pik'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercice(\n",
    "    2,\n",
    "    \"Change le debut du mot\",\n",
    "    'Change <code>mon_debut</code> ci-dessous (essaie <code>\".bul\"</code>, <code>\".evo\"</code> ou <code>\".dra\"</code>).',\n",
    "    \"Le modele predit des lettres differentes selon le debut.\",\n",
    ")\n",
    "\n",
    "# ==== MODIFIE ICI ====\n",
    "mon_debut = \".pik\"  # <-- Essaie \".bul\", \".evo\" ou \".dra\" !\n",
    "# ======================\n",
    "\n",
    "test_ids = [char_to_id[c] for c in mon_debut]\n",
    "probas = forward_llm(test_ids)\n",
    "top5 = sorted(range(VOCAB_SIZE), key=lambda i: -probas[i])[:5]\n",
    "\n",
    "print(f\"Predictions apres '{mon_debut}' :\")\n",
    "for idx in top5:\n",
    "    print(f\"  '{id_to_char[idx]}' : {probas[idx]:.1%}\")\n",
    "print()\n",
    "print(\"(Le modele n'est pas entraine, donc c'est du hasard !)\")\n",
    "print(\"(Apres entrainement dans la lecon 6, les predictions seront meilleures.)\")\n",
    "\n",
    "# Visualisation des predictions\n",
    "afficher_barres(\n",
    "    [probas[i] for i in top5],\n",
    "    [id_to_char[i] for i in top5],\n",
    "    titre=f\"Top 5 predictions apres '{mon_debut}'\",\n",
    ")\n",
    "\n",
    "# Visualisation des poids d'attention\n",
    "_attn = _calculer_poids_attention(mon_debut)\n",
    "afficher_attention(\n",
    "    _attn, list(mon_debut), titre=f\"Poids d'attention pour '{mon_debut}'\"\n",
    ")\n",
    "\n",
    "# Validation exercice 2\n",
    "verifier(\n",
    "    2,\n",
    "    mon_debut != \".pik\",\n",
    "    f\"Super ! Tu as teste les predictions apres '{mon_debut}'.\",\n",
    "    \"Change mon_debut pour un autre debut, par exemple '.bul' ou '.evo'.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement simplifié\n",
    "# (On utilise une méthode numérique pour les gradients,\n",
    "#  plus lente mais plus facile à comprendre)\n",
    "\n",
    "pokemons = [\n",
    "    \"arcanin\",\n",
    "    \"bulbizarre\",\n",
    "    \"carapuce\",\n",
    "    \"dracaufeu\",\n",
    "    \"ectoplasma\",\n",
    "    \"evoli\",\n",
    "    \"felinferno\",\n",
    "    \"gardevoir\",\n",
    "    \"goupix\",\n",
    "    \"lokhlass\",\n",
    "    \"lucario\",\n",
    "    \"metamorph\",\n",
    "    \"mewtwo\",\n",
    "    \"noctali\",\n",
    "    \"pikachu\",\n",
    "    \"rondoudou\",\n",
    "    \"ronflex\",\n",
    "    \"salameche\",\n",
    "    \"togepi\",\n",
    "    \"voltali\",\n",
    "]\n",
    "\n",
    "\n",
    "def calcul_loss(pokemons):\n",
    "    \"\"\"Calcule la loss moyenne sur tous les Pokémon.\"\"\"\n",
    "    loss_totale = 0\n",
    "    nb = 0\n",
    "    for pokemon in pokemons:\n",
    "        mot = \".\" + pokemon + \".\"\n",
    "        ids = [char_to_id[c] for c in mot]\n",
    "        for i in range(1, len(ids)):\n",
    "            seq = ids[:i]\n",
    "            cible = ids[i]\n",
    "            probas = forward_llm(seq[-CONTEXT:])\n",
    "            loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "            nb += 1\n",
    "    return loss_totale / nb\n",
    "\n",
    "\n",
    "loss_initiale = calcul_loss(pokemons)\n",
    "print(f\"Loss initiale : {loss_initiale:.3f}\")\n",
    "print(f\"(Loss d'un modèle parfaitement aléatoire : {math.log(VOCAB_SIZE):.3f})\")\n",
    "print()\n",
    "print(\"L'entraînement complet prendrait du temps en Python pur.\")\n",
    "print(\"C'est pour ça qu'en vrai on utilise PyTorch avec des GPU !\")\n",
    "print()\n",
    "print(\"Mais l'ARCHITECTURE est exactement la même que GPT-2, GPT-3, GPT-4.\")\n",
    "print(\"Seuls la taille et la puissance de calcul changent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération (même sans entraînement complet, on peut voir le mécanisme)\n",
    "\n",
    "\n",
    "def generer_llm(debut=\".\", temperature=1.0, max_len=15):\n",
    "    \"\"\"Génère un nom de Pokémon lettre par lettre avec notre mini-LLM.\"\"\"\n",
    "    ids = [char_to_id[c] for c in debut]\n",
    "    resultat = debut\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        probas = forward_llm(ids[-CONTEXT:])\n",
    "\n",
    "        # Température : < 1 = plus conservateur, > 1 = plus créatif\n",
    "        if temperature != 1.0:\n",
    "            logits = [math.log(p + 1e-10) / temperature for p in probas]\n",
    "            probas = softmax(logits)\n",
    "\n",
    "        # Choisir la prochaine lettre\n",
    "        idx = random.choices(range(VOCAB_SIZE), weights=probas, k=1)[0]\n",
    "\n",
    "        if idx == char_to_id[\".\"]:\n",
    "            break\n",
    "\n",
    "        ids.append(idx)\n",
    "        resultat += id_to_char[idx]\n",
    "\n",
    "    return resultat[1:] if resultat.startswith(\".\") else resultat\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Noms de Pokémon générés (modèle non-entraîné, juste pour montrer le mécanisme) :\"\n",
    ")\n",
    "print()\n",
    "for _ in range(10):\n",
    "    p = generer_llm(temperature=0.8)\n",
    "    print(f\"  {p.capitalize()}\")\n",
    "\n",
    "print()\n",
    "print(\"C'est du charabia car le modèle n'est pas entraîné.\")\n",
    "print(\"Mais le MÉCANISME est exactement celui de ChatGPT !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercice(\n",
    "    3,\n",
    "    \"Joue avec la temperature\",\n",
    "    \"Change <code>ma_temperature</code> ci-dessous (essaie 0.1 ou 2.0).\",\n",
    "    \"Temperature basse = repetitif. Temperature haute = creatif.\",\n",
    ")\n",
    "\n",
    "# ==== MODIFIE ICI ====\n",
    "ma_temperature = 0.8  # <-- Essaie 0.1 (sage) ou 2.0 (fou) !\n",
    "# ======================\n",
    "\n",
    "print(f\"Generation avec temperature = {ma_temperature} :\")\n",
    "print()\n",
    "for _ in range(10):\n",
    "    p = generer_llm(temperature=ma_temperature)\n",
    "    print(f\"  {p.capitalize()}\")\n",
    "print()\n",
    "if ma_temperature < 0.5:\n",
    "    print(\n",
    "        \"Temperature basse : le modele choisit toujours les lettres les plus probables.\"\n",
    "    )\n",
    "elif ma_temperature > 1.5:\n",
    "    print(\"Temperature haute : le modele explore des combinaisons inhabituelles !\")\n",
    "else:\n",
    "    print(\"Temperature moyenne : un bon equilibre entre creativite et coherence.\")\n",
    "\n",
    "# Validation exercice 3\n",
    "verifier(\n",
    "    3,\n",
    "    ma_temperature != 0.8,\n",
    "    f\"Genial ! Tu as explore la temperature {ma_temperature}.\",\n",
    "    \"Change ma_temperature pour une autre valeur, par exemple 0.1 ou 2.0.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Ce qu'on a appris\n\n```\nLeçon 1 : Compter les lettres qui suivent        -> bigramme\nLeçon 2 : Apprendre de ses erreurs                -> entraînement\nLeçon 3 : Regarder plusieurs lettres en arrière   -> embeddings + contexte\nLeçon 4 : Choisir les lettres importantes          -> attention\nLeçon 5 : Assembler le tout                       -> mini-LLM !\n```\n\n## La différence avec ChatGPT\n\n| | Notre mini-LLM | ChatGPT |\n|---|---|---|\n| Architecture | La même ! | La même ! |\n| Paramètres | ~2,800 | ~1,800,000,000,000 |\n| Données | 20 Pokémon | Internet entier |\n| Calcul | 1 PC, secondes | Des milliers de GPU, des mois |\n| Résultat | Noms de Pokémon inventés | Conversations, code, poésie... |\n\nL'algorithme est **le même**. La seule différence, c'est l'échelle.\n\n> *\"This file is the complete algorithm. Everything else is just efficiency.\"*\n> -- Andrej Karpathy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Pour aller plus loin\n\n- **microgpt.py** : Le code complet de Karpathy avec l'autograd et l'entraînement\n  [Lien](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95)\n\n- **Vidéo \"Let's build GPT\"** : Karpathy explique tout en 2h\n  [YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n\n- **nanoGPT** : Version avec PyTorch, entraînable pour de vrai\n  [GitHub](https://github.com/karpathy/nanoGPT)\n\n---\n\n**Félicitations ! Tu as compris comment fonctionne un LLM.**\n\n*Prochaine leçon : [06 - Entraîner le modèle](06_entrainer_le_modele.ipynb)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources (ISO 42001)\n",
    "\n",
    "- **Architecture complète GPT (embedding + attention + MLP + softmax)** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy\n",
    "- **Comparaison des paramètres GPT-4** : estimations publiques basées sur les rapports techniques OpenAI\n",
    "- **Explication du forward pass complet** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023)\n",
    "- **Concept de température pour la génération** : même source, section sampling\n",
    "- **\"Attention Is All You Need\"** : Vaswani et al., 2017, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "- **Dataset Pokémon** : (c) Nintendo / Creatures Inc. / GAME FREAK inc., usage éducatif. Source : [PokéAPI](https://pokeapi.co/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
