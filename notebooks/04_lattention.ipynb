{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecon 4 : L'attention\n",
    "\n",
    "## L'ingredient secret des GPT\n",
    "\n",
    "Imagine que tu lis la phrase : \"Le **chat** noir dort sur le **canape**.\"\n",
    "\n",
    "Si on te demande \"Ou dort le chat ?\", ton cerveau fait automatiquement\n",
    "le lien entre \"dort\", \"chat\" et \"canape\" -- meme si ces mots ne sont\n",
    "pas cote a cote.\n",
    "\n",
    "C'est exactement ce que fait le **mecanisme d'attention** :\n",
    "il permet au modele de regarder **n'importe quel** element du passe,\n",
    "pas seulement les derniers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment ca marche ?\n",
    "\n",
    "Pour chaque lettre, le modele se pose 3 questions :\n",
    "\n",
    "1. **Query (Q)** : \"Qu'est-ce que je cherche ?\" (ce que cette lettre a besoin de savoir)\n",
    "2. **Key (K)** : \"Qu'est-ce que j'offre ?\" (ce que cette lettre peut apporter)\n",
    "3. **Value (V)** : \"Quelle info je transmets ?\" (l'information reelle)\n",
    "\n",
    "L'attention = comparer chaque Query avec toutes les Keys pour trouver\n",
    "les lettres les plus utiles, puis collecter leurs Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Simulons un mot simple\n",
    "mot = \"chat\"\n",
    "print(f\"Mot : '{mot}'\")\n",
    "print(f\"Positions : {list(enumerate(mot))}\")\n",
    "print()\n",
    "\n",
    "# Chaque lettre a un embedding (simplifie a 4 dimensions)\n",
    "DIM = 4\n",
    "emb = {\n",
    "    'c': [1.0, 0.0, 0.5, -0.3],\n",
    "    'h': [0.2, 0.8, -0.1, 0.5],\n",
    "    'a': [0.5, 0.3, 0.9, 0.1],\n",
    "    't': [-0.3, 0.6, 0.2, 0.8],\n",
    "}\n",
    "\n",
    "for c, v in emb.items():\n",
    "    print(f\"  '{c}' -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour simplifier, on utilise les embeddings directement comme Q, K, V\n",
    "# (en vrai, il y a des matrices de transformation)\n",
    "\n",
    "def produit_scalaire(a, b):\n",
    "    \"\"\"Mesure la similarite entre deux vecteurs.\"\"\"\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilites.\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "# Calculons l'attention pour la lettre 't' (derniere position)\n",
    "# Question : quelles lettres precedentes sont importantes pour predire\n",
    "# ce qui vient apres 't' dans 'chat' ?\n",
    "\n",
    "query = emb['t']  # Ce que 't' cherche\n",
    "\n",
    "# Comparer avec toutes les lettres precedentes (y compris elle-meme)\n",
    "scores = []\n",
    "for c in mot:\n",
    "    key = emb[c]  # Ce que chaque lettre offre\n",
    "    score = produit_scalaire(query, key) / math.sqrt(DIM)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Scores d'attention pour 't' :\")\n",
    "for c, s in zip(mot, scores):\n",
    "    print(f\"  '{c}' : {s:.2f}\")\n",
    "\n",
    "# Transformer en probabilites\n",
    "poids_attention = softmax(scores)\n",
    "print()\n",
    "print(\"Poids d'attention (apres softmax) :\")\n",
    "for c, w in zip(mot, poids_attention):\n",
    "    barre = '#' * int(w * 30)\n",
    "    print(f\"  '{c}' : {w:.1%} {barre}\")\n",
    "\n",
    "print()\n",
    "print(\"Le modele 'regarde' plus les lettres avec un poids eleve !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter l'information (somme ponderee des Values)\n",
    "\n",
    "resultat = [0.0] * DIM\n",
    "for c, w in zip(mot, poids_attention):\n",
    "    value = emb[c]\n",
    "    for d in range(DIM):\n",
    "        resultat[d] += w * value[d]\n",
    "\n",
    "print(\"Vecteur de sortie de l'attention :\")\n",
    "print(f\"  {[f'{x:.2f}' for x in resultat]}\")\n",
    "print()\n",
    "print(\"Ce vecteur combine l'information de toutes les lettres,\")\n",
    "print(\"en donnant plus de poids aux lettres les plus pertinentes.\")\n",
    "print()\n",
    "print(\"C'est cette information qui sera utilisee pour predire\")\n",
    "print(\"la lettre suivante !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention causale : pas de triche !\n",
    "\n",
    "Regle importante : quand le modele predit la lettre suivante,\n",
    "il **ne peut pas regarder le futur** -- seulement le passe.\n",
    "\n",
    "```\n",
    "Pour predire apres 'c' : peut regarder [c]\n",
    "Pour predire apres 'h' : peut regarder [c, h]\n",
    "Pour predire apres 'a' : peut regarder [c, h, a]\n",
    "Pour predire apres 't' : peut regarder [c, h, a, t]\n",
    "```\n",
    "\n",
    "On appelle ca le **masque causal**. C'est ce qui fait de GPT un modele\n",
    "**auto-regressif** : il genere un token a la fois, de gauche a droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisons le masque causal\n",
    "print(\"Masque causal pour 'chat' :\")\n",
    "print()\n",
    "print(\"          c    h    a    t\")\n",
    "for i, c in enumerate(mot):\n",
    "    row = \"\"\n",
    "    for j in range(len(mot)):\n",
    "        if j <= i:\n",
    "            row += \"  OK \"\n",
    "        else:\n",
    "            row += \"  -- \"\n",
    "    print(f\"  {c} : {row}\")\n",
    "\n",
    "print()\n",
    "print(\"OK = peut regarder, -- = interdit (c'est le futur)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tetes : regarder de plusieurs facons\n",
    "\n",
    "En pratique, on utilise **plusieurs tetes d'attention** en parallele.\n",
    "Chaque tete apprend a chercher un type d'information different :\n",
    "\n",
    "- Tete 1 : quelles lettres forment des syllabes ensemble ?\n",
    "- Tete 2 : quelle est la voyelle la plus recente ?\n",
    "- Tete 3 : est-ce que c'est un debut ou une fin de mot ?\n",
    "\n",
    "Les resultats de toutes les tetes sont combines pour une prediction finale.\n",
    "\n",
    "Dans microgpt.py de Karpathy, il y a **4 tetes** d'attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume visuel\n",
    "\n",
    "```\n",
    "Lettres d'entree :  [c] [h] [a] [t]\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Embeddings :        [---] [---] [---] [---]\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Attention :         Chaque lettre regarde les precedentes\n",
    "                    et collecte l'info importante\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Prediction :        Quelle est la prochaine lettre ?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce qu'on a appris\n",
    "\n",
    "- L'**attention** permet au modele de regarder toutes les lettres precedentes, pas juste la derniere\n",
    "- Ca fonctionne avec **Q** (je cherche), **K** (j'offre), **V** (mon info)\n",
    "- Le **masque causal** empeche de tricher en regardant le futur\n",
    "- Plusieurs **tetes** d'attention regardent des choses differentes en parallele\n",
    "\n",
    "### Derniere etape\n",
    "\n",
    "On a maintenant toutes les pieces du puzzle. Dans la prochaine lecon,\n",
    "on assemble tout pour construire un vrai mini-LLM !\n",
    "\n",
    "---\n",
    "*Prochaine lecon : [05 - Mon premier LLM](05_mon_premier_llm.ipynb)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
