{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leçon 4 : L'attention\n",
    "\n",
    "## L'ingrédient secret des GPT\n",
    "\n",
    "Imagine que tu lis la phrase : \"Le **chat** noir dort sur le **canapé**.\"\n",
    "\n",
    "Si on te demande \"Où dort le chat ?\", ton cerveau fait automatiquement\n",
    "le lien entre \"dort\", \"chat\" et \"canapé\" -- même si ces mots ne sont\n",
    "pas côte à côte.\n",
    "\n",
    "C'est exactement ce que fait le **mécanisme d'attention** :\n",
    "il permet au modèle de regarder **n'importe quel** élément du passé,\n",
    "pas seulement les derniers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment ça marche ?\n",
    "\n",
    "Pour chaque lettre, le modèle se pose 3 questions :\n",
    "\n",
    "1. **Query (Q)** : \"Qu'est-ce que je cherche ?\" (ce que cette lettre a besoin de savoir)\n",
    "2. **Key (K)** : \"Qu'est-ce que j'offre ?\" (ce que cette lettre peut apporter)\n",
    "3. **Value (V)** : \"Quelle info je transmets ?\" (l'information réelle)\n",
    "\n",
    "L'attention = comparer chaque Query avec toutes les Keys pour trouver\n",
    "les lettres les plus utiles, puis collecter leurs Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Simulons un mot simple\n",
    "mot = \"chat\"\n",
    "print(f\"Mot : '{mot}'\")\n",
    "print(f\"Positions : {list(enumerate(mot))}\")\n",
    "print()\n",
    "\n",
    "# Chaque lettre a un embedding (simplifié à 4 dimensions)\n",
    "DIM = 4\n",
    "emb = {\n",
    "    'c': [1.0, 0.0, 0.5, -0.3],\n",
    "    'h': [0.2, 0.8, -0.1, 0.5],\n",
    "    'a': [0.5, 0.3, 0.9, 0.1],\n",
    "    't': [-0.3, 0.6, 0.2, 0.8],\n",
    "}\n",
    "\n",
    "for c, v in emb.items():\n",
    "    print(f\"  '{c}' -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour simplifier, on utilise les embeddings directement comme Q, K, V\n",
    "# (en vrai, il y a des matrices de transformation)\n",
    "\n",
    "def produit_scalaire(a, b):\n",
    "    \"\"\"Mesure la similarité entre deux vecteurs.\"\"\"\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilités.\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "# Calculons l'attention pour la lettre 't' (dernière position)\n",
    "# Question : quelles lettres précédentes sont importantes pour prédire\n",
    "# ce qui vient après 't' dans 'chat' ?\n",
    "\n",
    "query = emb['t']  # Ce que 't' cherche\n",
    "\n",
    "# Comparer avec toutes les lettres précédentes (y compris elle-même)\n",
    "scores = []\n",
    "for c in mot:\n",
    "    key = emb[c]  # Ce que chaque lettre offre\n",
    "    score = produit_scalaire(query, key) / math.sqrt(DIM)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Scores d'attention pour 't' :\")\n",
    "for c, s in zip(mot, scores):\n",
    "    print(f\"  '{c}' : {s:.2f}\")\n",
    "\n",
    "# Transformer en probabilités\n",
    "poids_attention = softmax(scores)\n",
    "print()\n",
    "print(\"Poids d'attention (après softmax) :\")\n",
    "for c, w in zip(mot, poids_attention):\n",
    "    barre = '#' * int(w * 30)\n",
    "    print(f\"  '{c}' : {w:.1%} {barre}\")\n",
    "\n",
    "print()\n",
    "print(\"Le modèle 'regarde' plus les lettres avec un poids élevé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter l'information (somme pondérée des Values)\n",
    "\n",
    "resultat = [0.0] * DIM\n",
    "for c, w in zip(mot, poids_attention):\n",
    "    value = emb[c]\n",
    "    for d in range(DIM):\n",
    "        resultat[d] += w * value[d]\n",
    "\n",
    "print(\"Vecteur de sortie de l'attention :\")\n",
    "print(f\"  {[f'{x:.2f}' for x in resultat]}\")\n",
    "print()\n",
    "print(\"Ce vecteur combine l'information de toutes les lettres,\")\n",
    "print(\"en donnant plus de poids aux lettres les plus pertinentes.\")\n",
    "print()\n",
    "print(\"C'est cette information qui sera utilisée pour prédire\")\n",
    "print(\"la lettre suivante !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention causale : pas de triche !\n",
    "\n",
    "Règle importante : quand le modèle prédit la lettre suivante,\n",
    "il **ne peut pas regarder le futur** -- seulement le passé.\n",
    "\n",
    "```\n",
    "Pour prédire après 'c' : peut regarder [c]\n",
    "Pour prédire après 'h' : peut regarder [c, h]\n",
    "Pour prédire après 'a' : peut regarder [c, h, a]\n",
    "Pour prédire après 't' : peut regarder [c, h, a, t]\n",
    "```\n",
    "\n",
    "On appelle ça le **masque causal**. C'est ce qui fait de GPT un modèle\n",
    "**auto-régressif** : il génère un token à la fois, de gauche à droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisons le masque causal\n",
    "print(\"Masque causal pour 'chat' :\")\n",
    "print()\n",
    "print(\"          c    h    a    t\")\n",
    "for i, c in enumerate(mot):\n",
    "    row = \"\"\n",
    "    for j in range(len(mot)):\n",
    "        if j <= i:\n",
    "            row += \"  OK \"\n",
    "        else:\n",
    "            row += \"  -- \"\n",
    "    print(f\"  {c} : {row}\")\n",
    "\n",
    "print()\n",
    "print(\"OK = peut regarder, -- = interdit (c'est le futur)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-têtes : regarder de plusieurs façons\n",
    "\n",
    "En pratique, on utilise **plusieurs têtes d'attention** en parallèle.\n",
    "Chaque tête apprend à chercher un type d'information différent :\n",
    "\n",
    "- Tête 1 : quelles lettres forment des syllabes ensemble ?\n",
    "- Tête 2 : quelle est la voyelle la plus récente ?\n",
    "- Tête 3 : est-ce que c'est un début ou une fin de mot ?\n",
    "\n",
    "Les résultats de toutes les têtes sont combinés pour une prédiction finale.\n",
    "\n",
    "Dans microgpt.py de Karpathy, il y a **4 têtes** d'attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé visuel\n",
    "\n",
    "```\n",
    "Lettres d'entrée :  [c] [h] [a] [t]\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Embeddings :        [---] [---] [---] [---]\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Attention :         Chaque lettre regarde les précédentes\n",
    "                    et collecte l'info importante\n",
    "                     |   |   |   |\n",
    "                     v   v   v   v\n",
    "Prédiction :        Quelle est la prochaine lettre ?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce qu'on a appris\n",
    "\n",
    "- L'**attention** permet au modèle de regarder toutes les lettres précédentes, pas juste la dernière\n",
    "- Ça fonctionne avec **Q** (je cherche), **K** (j'offre), **V** (mon info)\n",
    "- Le **masque causal** empêche de tricher en regardant le futur\n",
    "- Plusieurs **têtes** d'attention regardent des choses différentes en parallèle\n",
    "\n",
    "### Dernière étape\n",
    "\n",
    "On a maintenant toutes les pièces du puzzle. Dans la prochaine leçon,\n",
    "on assemble tout pour construire un vrai mini-LLM !\n",
    "\n",
    "---\n",
    "*Prochaine leçon : [05 - Mon premier LLM](05_mon_premier_llm.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Sources (ISO 42001)\n\n- **Mécanisme Q/K/V et masque causal** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, section self-attention\n- **\"Attention Is All You Need\"** : Vaswani et al., 2017, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) — article fondateur des Transformers\n- **Explication visuelle de l'attention** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023), section attention\n- **Multi-head attention (4 têtes dans microgpt.py)** : même source, paramètre `n_head=4`",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}