{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "> **Rappel** : clique sur une cellule grise, puis **Shift + Entree** pour l'executer.\n> Execute les cellules **dans l'ordre** de haut en bas.\n\n---\n\n# Leçon 4 : L'attention\n\n## L'ingrédient secret des GPT\n\nImagine que tu lis la phrase : \"Le **chat** noir dort sur le **canapé**.\"\n\nSi on te demande \"Où dort le chat ?\", ton cerveau fait automatiquement\nle lien entre \"dort\", \"chat\" et \"canapé\" -- même si ces mots ne sont\npas côte à côte.\n\nC'est exactement ce que fait le **mécanisme d'attention** :\nil permet au modèle de regarder **n'importe quel** élément du passé,\npas seulement les derniers."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Comment ça marche ?\n\nPour chaque lettre, le modèle se pose 3 questions :\n\n1. **Query (Q)** : \"Qu'est-ce que je cherche ?\" (ce que cette lettre a besoin de savoir)\n2. **Key (K)** : \"Qu'est-ce que j'offre ?\" (ce que cette lettre peut apporter)\n3. **Value (V)** : \"Quelle info je transmets ?\" (l'information réelle)\n\nL'attention = comparer chaque Query avec toutes les Keys pour trouver\nles lettres les plus utiles, puis collecter leurs Values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Simulons un mot simple\n",
    "mot = \"chat\"\n",
    "print(f\"Mot : '{mot}'\")\n",
    "print(f\"Positions : {list(enumerate(mot))}\")\n",
    "print()\n",
    "\n",
    "# Chaque lettre a un embedding (simplifié à 4 dimensions)\n",
    "DIM = 4\n",
    "emb = {\n",
    "    \"c\": [1.0, 0.0, 0.5, -0.3],\n",
    "    \"h\": [0.2, 0.8, -0.1, 0.5],\n",
    "    \"a\": [0.5, 0.3, 0.9, 0.1],\n",
    "    \"t\": [-0.3, 0.6, 0.2, 0.8],\n",
    "}\n",
    "\n",
    "for c, v in emb.items():\n",
    "    print(f\"  '{c}' -> {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour simplifier, on utilise les embeddings directement comme Q, K, V\n",
    "# (en vrai, il y a des matrices de transformation)\n",
    "\n",
    "\n",
    "def produit_scalaire(a, b):\n",
    "    \"\"\"Mesure la similarité entre deux vecteurs.\"\"\"\n",
    "    return sum(x * y for x, y in zip(a, b, strict=False))\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"Transforme les scores en probabilités.\"\"\"\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "\n",
    "# Calculons l'attention pour la lettre 't' (dernière position)\n",
    "# Question : quelles lettres précédentes sont importantes pour prédire\n",
    "# ce qui vient après 't' dans 'chat' ?\n",
    "\n",
    "query = emb[\"t\"]  # Ce que 't' cherche\n",
    "\n",
    "# Comparer avec toutes les lettres précédentes (y compris elle-même)\n",
    "scores = []\n",
    "for c in mot:\n",
    "    key = emb[c]  # Ce que chaque lettre offre\n",
    "    score = produit_scalaire(query, key) / math.sqrt(DIM)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"Scores d'attention pour 't' :\")\n",
    "for c, s in zip(mot, scores, strict=False):\n",
    "    print(f\"  '{c}' : {s:.2f}\")\n",
    "\n",
    "# Transformer en probabilités\n",
    "poids_attention = softmax(scores)\n",
    "print()\n",
    "print(\"Poids d'attention (après softmax) :\")\n",
    "for c, w in zip(mot, poids_attention, strict=False):\n",
    "    barre = \"#\" * int(w * 30)\n",
    "    print(f\"  '{c}' : {w:.1%} {barre}\")\n",
    "\n",
    "print()\n",
    "print(\"Le modèle 'regarde' plus les lettres avec un poids élevé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 1)\n\nDans la cellule ci-dessous, change `ma_lettre` pour voir comment\nl'attention change selon la lettre qui \"pose la question\".\nEssaie `\"c\"`, `\"h\"` ou `\"a\"` -- les poids d'attention seront differents !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 1 : Change la lettre qui pose la question, puis Shift + Entree ---\n",
    "ma_lettre = \"t\"  # <-- Essaie \"c\", \"h\" ou \"a\" !\n",
    "\n",
    "if ma_lettre not in emb:\n",
    "    print(f\"Erreur : '{ma_lettre}' n'est pas dans le mot 'chat'.\")\n",
    "    print(\"Choisis c, h, a ou t.\")\n",
    "else:\n",
    "    query_test = emb[ma_lettre]\n",
    "    scores_test = []\n",
    "    for c in mot:\n",
    "        key = emb[c]\n",
    "        score = produit_scalaire(query_test, key) / math.sqrt(DIM)\n",
    "        scores_test.append(score)\n",
    "    poids_test = softmax(scores_test)\n",
    "    print(f\"Poids d'attention quand '{ma_lettre}' pose la question :\")\n",
    "    for c, w in zip(mot, poids_test, strict=False):\n",
    "        barre = \"#\" * int(w * 30)\n",
    "        print(f\"  '{c}' : {w:.1%} {barre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter l'information (somme pondérée des Values)\n",
    "\n",
    "resultat = [0.0] * DIM\n",
    "for c, w in zip(mot, poids_attention, strict=False):\n",
    "    value = emb[c]\n",
    "    for d in range(DIM):\n",
    "        resultat[d] += w * value[d]\n",
    "\n",
    "print(\"Vecteur de sortie de l'attention :\")\n",
    "print(f\"  {[f'{x:.2f}' for x in resultat]}\")\n",
    "print()\n",
    "print(\"Ce vecteur combine l'information de toutes les lettres,\")\n",
    "print(\"en donnant plus de poids aux lettres les plus pertinentes.\")\n",
    "print()\n",
    "print(\"C'est cette information qui sera utilisée pour prédire\")\n",
    "print(\"la lettre suivante !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Attention causale : pas de triche !\n\nRègle importante : quand le modèle prédit la lettre suivante,\nil **ne peut pas regarder le futur** -- seulement le passé.\n\n```\nPour prédire après 'c' : peut regarder [c]\nPour prédire après 'h' : peut regarder [c, h]\nPour prédire après 'a' : peut regarder [c, h, a]\nPour prédire après 't' : peut regarder [c, h, a, t]\n```\n\nOn appelle ça le **masque causal**. C'est ce qui fait de GPT un modèle\n**auto-régressif** : il génère un token à la fois, de gauche à droite."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisons le masque causal\n",
    "print(\"Masque causal pour 'chat' :\")\n",
    "print()\n",
    "print(\"          c    h    a    t\")\n",
    "for i, c in enumerate(mot):\n",
    "    row = \"\"\n",
    "    for j in range(len(mot)):\n",
    "        if j <= i:\n",
    "            row += \"  OK \"\n",
    "        else:\n",
    "            row += \"  -- \"\n",
    "    print(f\"  {c} : {row}\")\n",
    "\n",
    "print()\n",
    "print(\"OK = peut regarder, -- = interdit (c'est le futur)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 2)\n\nDans la cellule ci-dessous, change `mon_mot` pour voir le masque causal\nd'un mot plus long. Essaie `\"plume\"`, `\"arbre\"` ou ton prenom (a-z) !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 2 : Change le mot pour voir le masque causal, puis Shift + Entree ---\n",
    "mon_mot = \"chat\"  # <-- Essaie \"plume\", \"arbre\" ou ton prenom !\n",
    "\n",
    "print(f\"Masque causal pour '{mon_mot}' :\")\n",
    "print()\n",
    "header = \"     \" + \"\".join(f\"  {c}  \" for c in mon_mot)\n",
    "print(header)\n",
    "for i, c in enumerate(mon_mot):\n",
    "    row = \"\"\n",
    "    for j in range(len(mon_mot)):\n",
    "        if j <= i:\n",
    "            row += \"  OK \"\n",
    "        else:\n",
    "            row += \"  -- \"\n",
    "    print(f\"  {c} : {row}\")\n",
    "print()\n",
    "print(\"Chaque lettre ne voit que les lettres avant elle (et elle-meme).\")\n",
    "print(f\"Le triangle grandit avec la longueur du mot ({len(mon_mot)} lettres ici).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Multi-têtes : regarder de plusieurs façons\n\nEn pratique, on utilise **plusieurs têtes d'attention** en parallèle.\nChaque tête apprend à chercher un type d'information différent :\n\n- Tête 1 : quelles lettres forment des syllabes ensemble ?\n- Tête 2 : quelle est la voyelle la plus récente ?\n- Tête 3 : est-ce que c'est un début ou une fin de mot ?\n\nLes résultats de toutes les têtes sont combinés pour une prédiction finale.\n\nDans microgpt.py de Karpathy, il y a **4 têtes** d'attention."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Résumé visuel\n\n```\nLettres d'entrée :  [c] [h] [a] [t]\n                     |   |   |   |\n                     v   v   v   v\nEmbeddings :        [---] [---] [---] [---]\n                     |   |   |   |\n                     v   v   v   v\nAttention :         Chaque lettre regarde les précédentes\n                    et collecte l'info importante\n                     |   |   |   |\n                     v   v   v   v\nPrédiction :        Quelle est la prochaine lettre ?\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Ce qu'on a appris\n\n- L'**attention** permet au modèle de regarder toutes les lettres précédentes, pas juste la dernière\n- Ça fonctionne avec **Q** (je cherche), **K** (j'offre), **V** (mon info)\n- Le **masque causal** empêche de tricher en regardant le futur\n- Plusieurs **têtes** d'attention regardent des choses différentes en parallèle\n\n### Dernière étape\n\nOn a maintenant toutes les pièces du puzzle. Dans la prochaine leçon,\non assemble tout pour construire un vrai mini-LLM !\n\n---\n*Prochaine leçon : [05 - Mon premier LLM](05_mon_premier_llm.ipynb)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources (ISO 42001)\n",
    "\n",
    "- **Mécanisme Q/K/V et masque causal** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) — Andrej Karpathy, section self-attention\n",
    "- **\"Attention Is All You Need\"** : Vaswani et al., 2017, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762) — article fondateur des Transformers\n",
    "- **Explication visuelle de l'attention** : [Vidéo \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) — Andrej Karpathy (2023), section attention\n",
    "- **Multi-head attention (4 têtes dans microgpt.py)** : même source, paramètre `n_head=4`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
