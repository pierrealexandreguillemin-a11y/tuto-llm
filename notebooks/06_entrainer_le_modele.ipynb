{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": "> **Rappel** : clique sur une cellule grise, puis **Shift + Entree** pour l'executer.\n> Execute les cellules **dans l'ordre** de haut en bas.\n\n---\n\n# Lecon 6 : Entrainer le modele\n\n## Le moment de verite !\n\nDans la lecon 5, on a construit un mini-LLM complet : embeddings,\nattention, MLP, softmax. Mais ses poids etaient **aleatoires** --\nil ne savait rien et generait du charabia.\n\nAujourd'hui, on va lui **apprendre** a generer des noms de Pokemon.\n\nC'est comme un joueur qui decouvre des centaines de Pokemon et finit\npar comprendre \"comment ca sonne\", un nom de Pokemon.\n\n> L'entrainement va prendre quelques minutes. Pendant que le modele\n> apprend, tu pourras lire les sections \"En vrai...\" plus bas !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "_exercices_faits = set()\n",
    "_NB_TOTAL = 3\n",
    "\n",
    "\n",
    "def verifier(num_exercice, condition, message_ok, message_aide=\"\"):\n",
    "    \"\"\"Valide un exercice avec feedback HTML vert/rouge + compteur.\"\"\"\n",
    "    if condition:\n",
    "        _exercices_faits.add(num_exercice)\n",
    "        n = len(_exercices_faits)\n",
    "        barre = \"\\U0001f7e9\" * n + \"\\u2b1c\" * (_NB_TOTAL - n)\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<div style=\"padding:10px;background:#d4edda;border-left:4px solid #28a745;'\n",
    "                f'margin:8px 0;border-radius:4px;font-family:sans-serif\">'\n",
    "                f\"\\u2705 <b>{message_ok}</b><br>\"\n",
    "                f'<span style=\"color:#555\">Progression : {barre} {n}/{_NB_TOTAL}</span></div>'\n",
    "            )\n",
    "        )\n",
    "        if n == _NB_TOTAL:\n",
    "            display(\n",
    "                HTML(\n",
    "                    '<div style=\"padding:12px;background:linear-gradient(135deg,#667eea,#764ba2);'\n",
    "                    \"color:white;border-radius:8px;text-align:center;font-family:sans-serif;\"\n",
    "                    'font-size:1.2em;margin:8px 0\">\\U0001f3c6 <b>Bravo ! Toutes les activites de cette lecon sont terminees !</b></div>'\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<div style=\"padding:10px;background:#fff3cd;border-left:4px solid #ffc107;'\n",
    "                f'margin:8px 0;border-radius:4px;font-family:sans-serif\">'\n",
    "                f\"\\U0001f4a1 <b>{message_aide}</b></div>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def afficher_evolution_loss(pertes, titre=\"Courbe de loss\"):\n",
    "    \"\"\"Affiche la loss sous forme de barres verticales HTML avec degrade.\"\"\"\n",
    "    if not pertes:\n",
    "        return\n",
    "    max_loss = max(pertes)\n",
    "    min_loss = min(pertes)\n",
    "    bars = \"\"\n",
    "    n = len(pertes)\n",
    "    bar_w = max(4, min(40, 600 // n))\n",
    "    for i, loss in enumerate(pertes):\n",
    "        h = int((loss / max_loss) * 120) if max_loss > 0 else 0\n",
    "        ratio = (loss - min_loss) / (max_loss - min_loss) if max_loss > min_loss else 0\n",
    "        r = int(220 * ratio + 30)\n",
    "        g = int(180 * (1 - ratio) + 50)\n",
    "        bars += (\n",
    "            f'<div style=\"display:inline-block;width:{bar_w}px;vertical-align:bottom;'\n",
    "            f'margin:0 1px;height:{h}px;background:rgb({r},{g},60);border-radius:2px 2px 0 0\" '\n",
    "            f'title=\"Epoch {i + 1}: {loss:.3f}\"></div>'\n",
    "        )\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<!-- tuto-viz --><div style=\"margin:8px 0\"><b>{titre}</b>'\n",
    "            f'<div style=\"display:flex;align-items:flex-end;height:140px;padding:8px;'\n",
    "            f'background:#f8f9fa;border-radius:4px;margin-top:4px\">{bars}</div>'\n",
    "            f'<div style=\"display:flex;justify-content:space-between;color:#555;font-size:0.8em;margin-top:2px\">'\n",
    "            f\"<span>Epoch 1 (loss={pertes[0]:.2f})</span><span>Epoch {len(pertes)} (loss={pertes[-1]:.2f})</span></div></div>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def afficher_barres(valeurs, etiquettes, titre=\"Probabilites\"):\n",
    "    \"\"\"Affiche des barres horizontales HTML.\"\"\"\n",
    "    rows = \"\"\n",
    "    max_val = max(valeurs) if valeurs else 1\n",
    "    for etiq, val in zip(etiquettes, valeurs, strict=False):\n",
    "        pct = val / max_val * 100 if max_val > 0 else 0\n",
    "        rows += (\n",
    "            f'<tr><td style=\"padding:3px 8px;font-weight:bold;font-size:1em\">{etiq}</td>'\n",
    "            f'<td style=\"padding:3px;width:300px\"><div style=\"background:linear-gradient(90deg,#667eea,#764ba2);'\n",
    "            f'width:{max(pct, 2):.0f}%;height:20px;border-radius:4px\"></div></td>'\n",
    "            f'<td style=\"padding:3px 8px;font-size:0.9em\">{val:.1%}</td></tr>'\n",
    "        )\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<!-- tuto-viz --><div style=\"margin:8px 0\"><b>{titre}</b>'\n",
    "            f'<table style=\"border-collapse:collapse;margin-top:4px\">{rows}</table></div>'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Outils de visualisation charges !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": "---\n## Etape 1 : Charger les noms de Pokemon\n\nDans les lecons precedentes, on utilisait une poignee de Pokemon\necrits a la main. Maintenant, on utilise un vrai dataset :\n\n**~1 000 noms de Pokemon** tires de la PokeAPI ((c) Nintendo)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Noms de Pokemon (c) Nintendo / Creatures Inc. / GAME FREAK inc.\n",
    "# Source : PokeAPI (https://pokeapi.co/) -- usage educatif uniquement\n",
    "_POKEMON_DATA = \"\"\"\n",
    "abo\n",
    "abra\n",
    "absol\n",
    "aeromite\n",
    "aeropteryx\n",
    "aflamanoir\n",
    "airmure\n",
    "akwakwak\n",
    "alakazam\n",
    "aligatueur\n",
    "altaria\n",
    "amaama\n",
    "amagara\n",
    "amassel\n",
    "amonistar\n",
    "amonita\n",
    "amovenus\n",
    "amphinobi\n",
    "ampibidou\n",
    "anchwatt\n",
    "angoliath\n",
    "anorith\n",
    "apireine\n",
    "apitrini\n",
    "aquali\n",
    "arakdo\n",
    "araqua\n",
    "arbok\n",
    "arboliva\n",
    "arcanin\n",
    "arceus\n",
    "archeduc\n",
    "archeodong\n",
    "archeomire\n",
    "arcko\n",
    "argouste\n",
    "arkeapti\n",
    "armaldo\n",
    "armulys\n",
    "arrozard\n",
    "artikodin\n",
    "aspicot\n",
    "astronelle\n",
    "avaltout\n",
    "axoloto\n",
    "azumarill\n",
    "azurill\n",
    "babimanta\n",
    "bacabouh\n",
    "badabouin\n",
    "baggaid\n",
    "baggiguane\n",
    "balbaleze\n",
    "balbuto\n",
    "balignon\n",
    "bamboiselle\n",
    "banshitrouye\n",
    "baojian\n",
    "barbicha\n",
    "bargantua\n",
    "barloche\n",
    "barpau\n",
    "bastiodon\n",
    "batracne\n",
    "baudrive\n",
    "bazoucan\n",
    "bebecaille\n",
    "bekaglacon\n",
    "bekipan\n",
    "beldeneige\n",
    "berasca\n",
    "berserkatt\n",
    "betochef\n",
    "bibichut\n",
    "blancoton\n",
    "bleuseille\n",
    "blindalys\n",
    "blindepique\n",
    "blizzaroi\n",
    "blizzeval\n",
    "blizzi\n",
    "boguerisse\n",
    "bombydou\n",
    "boreas\n",
    "boskara\n",
    "bouldeneu\n",
    "boumata\n",
    "bourrinos\n",
    "boustiflor\n",
    "braisillon\n",
    "branette\n",
    "brasegali\n",
    "brindibou\n",
    "briochien\n",
    "brocelome\n",
    "brouhabam\n",
    "brutalibre\n",
    "brutapode\n",
    "bruyverne\n",
    "bulbizarre\n",
    "cablifere\n",
    "cabriolaine\n",
    "cacnea\n",
    "cacturne\n",
    "cadoizo\n",
    "camerupt\n",
    "canarbello\n",
    "canarticho\n",
    "cancrelove\n",
    "candine\n",
    "caninos\n",
    "capidextre\n",
    "capumain\n",
    "carabaffe\n",
    "carabing\n",
    "carapagos\n",
    "carapuce\n",
    "caratroc\n",
    "carchacrok\n",
    "carmache\n",
    "carmadura\n",
    "carvanha\n",
    "castorno\n",
    "celebi\n",
    "cerbyllin\n",
    "cerfrousse\n",
    "ceribou\n",
    "ceriflor\n",
    "chacripan\n",
    "chaffreux\n",
    "chaglam\n",
    "chamallot\n",
    "chapignon\n",
    "chapotus\n",
    "charbambin\n",
    "charbi\n",
    "charibari\n",
    "charkos\n",
    "charmillon\n",
    "charmilly\n",
    "charmina\n",
    "charpenti\n",
    "chartor\n",
    "chefdefer\n",
    "chelours\n",
    "chenipan\n",
    "chenipotte\n",
    "cheniselle\n",
    "cheniti\n",
    "chetiflor\n",
    "chevroum\n",
    "chimpenfeu\n",
    "chinchidou\n",
    "chlorobule\n",
    "chochodile\n",
    "chongjian\n",
    "chovsourir\n",
    "chrysacier\n",
    "chrysapile\n",
    "chuchmur\n",
    "cizayox\n",
    "clamiral\n",
    "cleopsytra\n",
    "clic\n",
    "cliticlic\n",
    "coatox\n",
    "cobaltium\n",
    "cochignon\n",
    "coconfort\n",
    "cocotine\n",
    "coiffeton\n",
    "coleodome\n",
    "colhomard\n",
    "colimucus\n",
    "colombeau\n",
    "colossinge\n",
    "compagnol\n",
    "concombaffe\n",
    "coquiperl\n",
    "corayome\n",
    "corayon\n",
    "corboss\n",
    "cornebre\n",
    "corvaillus\n",
    "cosmog\n",
    "cosmovum\n",
    "cotovol\n",
    "couafarel\n",
    "couaneton\n",
    "coudlangue\n",
    "coupenotte\n",
    "courrousinge\n",
    "couverdure\n",
    "coxy\n",
    "coxyclaque\n",
    "crabagarre\n",
    "crabaraque\n",
    "crabicoque\n",
    "crabominable\n",
    "cradopaud\n",
    "craparoi\n",
    "crapustule\n",
    "crefadet\n",
    "crefollet\n",
    "crehelf\n",
    "cremy\n",
    "cresselia\n",
    "crikzik\n",
    "croaporal\n",
    "crocogril\n",
    "crocorible\n",
    "crocrodil\n",
    "croquine\n",
    "crustabri\n",
    "cryodo\n",
    "cryptero\n",
    "cupcanaille\n",
    "dardargnan\n",
    "darkrai\n",
    "darumacho\n",
    "darumarond\n",
    "debugant\n",
    "dedenne\n",
    "deflaisan\n",
    "delcatty\n",
    "delestin\n",
    "demanta\n",
    "demeteros\n",
    "demolosse\n",
    "denticrisse\n",
    "deoxys\n",
    "desseliande\n",
    "deusolourdo\n",
    "dialga\n",
    "diamat\n",
    "diancie\n",
    "dimocles\n",
    "dimoret\n",
    "dinglu\n",
    "dinoclier\n",
    "dispareptil\n",
    "dodoala\n",
    "dodrio\n",
    "doduo\n",
    "dofin\n",
    "dogrino\n",
    "dolman\n",
    "donphan\n",
    "doudouvet\n",
    "draby\n",
    "dracaufeu\n",
    "drackhaus\n",
    "draco\n",
    "dracolosse\n",
    "dragmara\n",
    "draieul\n",
    "drakkarmin\n",
    "drascore\n",
    "dratatin\n",
    "drattak\n",
    "dunaconda\n",
    "dunaja\n",
    "duralugon\n",
    "dynavolt\n",
    "ecaid\n",
    "ecayon\n",
    "ecrapince\n",
    "ecremeuh\n",
    "ectoplasma\n",
    "effleche\n",
    "ekaiser\n",
    "elecsprint\n",
    "electhor\n",
    "electrode\n",
    "elekable\n",
    "elekid\n",
    "elektek\n",
    "embrochet\n",
    "embrylex\n",
    "emolga\n",
    "empiflor\n",
    "engloutyran\n",
    "entei\n",
    "eoko\n",
    "epinedefer\n",
    "escargaume\n",
    "escroco\n",
    "ethernatos\n",
    "etouraptor\n",
    "etourmi\n",
    "etourvol\n",
    "evoli\n",
    "exagide\n",
    "excavarenne\n",
    "excelangue\n",
    "famignol\n",
    "fantominus\n",
    "fantyrm\n",
    "farfaduvet\n",
    "farfuret\n",
    "farfurex\n",
    "farigiraf\n",
    "favianos\n",
    "felicanis\n",
    "felinferno\n",
    "ferdeter\n",
    "fermite\n",
    "ferosinge\n",
    "feuforeve\n",
    "feuillajou\n",
    "feuiloutan\n",
    "feunard\n",
    "feunnec\n",
    "feupercant\n",
    "feurisson\n",
    "filentrappe\n",
    "flabebe\n",
    "flagadoss\n",
    "flamajou\n",
    "flambino\n",
    "flambusard\n",
    "flamenroule\n",
    "flamiaou\n",
    "flamigator\n",
    "flamoutan\n",
    "flingouste\n",
    "flobio\n",
    "floette\n",
    "floramantis\n",
    "floravol\n",
    "floreclat\n",
    "florges\n",
    "florizarre\n",
    "flotajou\n",
    "flotillon\n",
    "flotoutan\n",
    "flottemeche\n",
    "fluvetin\n",
    "fongusfurie\n",
    "foretress\n",
    "forgelina\n",
    "forgella\n",
    "forgerette\n",
    "fortivoire\n",
    "fortusimia\n",
    "fouinar\n",
    "fouinette\n",
    "fourbelin\n",
    "fragilady\n",
    "fragroin\n",
    "frigodo\n",
    "frison\n",
    "frissonille\n",
    "froussardine\n",
    "fulgudog\n",
    "fulgulairo\n",
    "fulguris\n",
    "funecire\n",
    "furaiglon\n",
    "galegon\n",
    "galekid\n",
    "galeking\n",
    "galifeu\n",
    "gallame\n",
    "galopa\n",
    "galvagla\n",
    "galvagon\n",
    "galvaran\n",
    "gambex\n",
    "gamblast\n",
    "gardedefer\n",
    "gardevoir\n",
    "gaulet\n",
    "genesect\n",
    "geolithe\n",
    "germeclat\n",
    "germignon\n",
    "gigalithe\n",
    "gigansel\n",
    "girafarig\n",
    "giratina\n",
    "givrali\n",
    "glaivodo\n",
    "gloupti\n",
    "gobou\n",
    "goelise\n",
    "goinfrex\n",
    "golemastoc\n",
    "golgopathe\n",
    "gorythmic\n",
    "goupelin\n",
    "goupilou\n",
    "goupix\n",
    "gourmelet\n",
    "gouroutan\n",
    "grahyena\n",
    "grainipiot\n",
    "granbull\n",
    "granivol\n",
    "gravalanch\n",
    "grelacon\n",
    "grenousse\n",
    "gribouraigne\n",
    "griknot\n",
    "grillepattes\n",
    "grimalin\n",
    "grindur\n",
    "gringolem\n",
    "grodoudou\n",
    "grodrive\n",
    "grolem\n",
    "gromago\n",
    "grondogue\n",
    "groret\n",
    "grotadmorv\n",
    "grotichon\n",
    "groudon\n",
    "gruikui\n",
    "gueriaigle\n",
    "guerilande\n",
    "hachecateur\n",
    "hariyama\n",
    "hastacuda\n",
    "haydaim\n",
    "heatran\n",
    "heledelle\n",
    "heliatronc\n",
    "helionceau\n",
    "herbizarre\n",
    "hericendre\n",
    "hexadron\n",
    "hexagel\n",
    "hippodocus\n",
    "hippopotas\n",
    "hooh\n",
    "hoopa\n",
    "hoothoot\n",
    "hottedefer\n",
    "hurlequeue\n",
    "hydragla\n",
    "hydragon\n",
    "hypnomade\n",
    "hypocean\n",
    "hyporoi\n",
    "hypotrempe\n",
    "iguolta\n",
    "incisache\n",
    "insecateur\n",
    "insolourdo\n",
    "irefoudre\n",
    "ixon\n",
    "jirachi\n",
    "joliflor\n",
    "judokrak\n",
    "jungko\n",
    "kabuto\n",
    "kabutops\n",
    "kadabra\n",
    "kaiminus\n",
    "kaimorse\n",
    "kangourex\n",
    "kaorine\n",
    "kapoera\n",
    "karaclee\n",
    "katagami\n",
    "kecleon\n",
    "keldeo\n",
    "keunotor\n",
    "khelocrok\n",
    "kicklee\n",
    "kirlia\n",
    "kokiyas\n",
    "koraidon\n",
    "korillon\n",
    "krabboss\n",
    "krabby\n",
    "kraknoix\n",
    "krakos\n",
    "kranidos\n",
    "kravarech\n",
    "kungfouine\n",
    "kyogre\n",
    "kyurem\n",
    "laggron\n",
    "lainergie\n",
    "lakmecygne\n",
    "lamantine\n",
    "lamperoie\n",
    "lampignon\n",
    "lancargot\n",
    "lanssorien\n",
    "lanturn\n",
    "laporeille\n",
    "lapyro\n",
    "larmeleon\n",
    "larvadar\n",
    "larveyette\n",
    "larvibule\n",
    "latias\n",
    "latios\n",
    "leboulerou\n",
    "leopardus\n",
    "lepidonille\n",
    "lestombaile\n",
    "leuphorie\n",
    "leveinard\n",
    "leviator\n",
    "lewsor\n",
    "lezargus\n",
    "lianaja\n",
    "libegon\n",
    "lilia\n",
    "lilliterelle\n",
    "limagma\n",
    "limaspeed\n",
    "limonde\n",
    "lineon\n",
    "lippouti\n",
    "lippoutou\n",
    "lixy\n",
    "lockpin\n",
    "lokhlass\n",
    "lombre\n",
    "lougaroc\n",
    "loupio\n",
    "lovdisc\n",
    "lucanon\n",
    "lucario\n",
    "ludicolo\n",
    "lugia\n",
    "lugulabre\n",
    "lumineon\n",
    "lumivole\n",
    "lunala\n",
    "luxio\n",
    "luxray\n",
    "machoc\n",
    "machopeur\n",
    "mackogneur\n",
    "macronium\n",
    "maganon\n",
    "magby\n",
    "magearna\n",
    "magicarpe\n",
    "magireve\n",
    "magmar\n",
    "magneti\n",
    "magneton\n",
    "magnezone\n",
    "majaspic\n",
    "makuhita\n",
    "malamandre\n",
    "malosse\n",
    "malvalame\n",
    "mamanbo\n",
    "mammochon\n",
    "manaphy\n",
    "mandrillon\n",
    "manglouton\n",
    "mangriff\n",
    "manternel\n",
    "manzai\n",
    "maracachi\n",
    "maraiste\n",
    "marcacrin\n",
    "marill\n",
    "marisson\n",
    "marshadow\n",
    "mascaiman\n",
    "maskadra\n",
    "massko\n",
    "mastouffe\n",
    "mateloutre\n",
    "matoufeu\n",
    "matourgeon\n",
    "medhyena\n",
    "meditikka\n",
    "meganium\n",
    "megapagos\n",
    "meios\n",
    "melancolux\n",
    "melmetal\n",
    "melo\n",
    "melodelfe\n",
    "meloetta\n",
    "melofee\n",
    "melokrik\n",
    "meltan\n",
    "mentali\n",
    "mesmerella\n",
    "metalosse\n",
    "metamorph\n",
    "metang\n",
    "meteno\n",
    "mew\n",
    "mewtwo\n",
    "mglaquette\n",
    "miamiasme\n",
    "miaouss\n",
    "miascarade\n",
    "miasmax\n",
    "migalos\n",
    "milobellus\n",
    "mimantis\n",
    "mimejr\n",
    "mimigal\n",
    "mimiqui\n",
    "mimitoss\n",
    "minidraco\n",
    "minisange\n",
    "minotaupe\n",
    "miradar\n",
    "miraidon\n",
    "mistigrix\n",
    "mitedefer\n",
    "mmime\n",
    "momartik\n",
    "monaflemit\n",
    "monorpale\n",
    "monthracite\n",
    "mordudor\n",
    "morpeko\n",
    "morpheo\n",
    "motisma\n",
    "motorizard\n",
    "moufflair\n",
    "moufouette\n",
    "moumouflon\n",
    "moumouton\n",
    "mouscoto\n",
    "moustillon\n",
    "moyade\n",
    "muciole\n",
    "mucuscule\n",
    "munja\n",
    "munna\n",
    "muplodocus\n",
    "mushana\n",
    "mustebouee\n",
    "musteflott\n",
    "mygavolt\n",
    "mysdibule\n",
    "mystherbe\n",
    "nanmeouie\n",
    "natu\n",
    "necrozma\n",
    "negapi\n",
    "neitram\n",
    "nemelios\n",
    "nenupiot\n",
    "nidoking\n",
    "nidoqueen\n",
    "nidoran\n",
    "nidorina\n",
    "nidorino\n",
    "nigirigon\n",
    "nigosier\n",
    "ningale\n",
    "ninjask\n",
    "nirondelle\n",
    "noacier\n",
    "noadkoko\n",
    "noarfang\n",
    "noctali\n",
    "noctunoir\n",
    "nodulithe\n",
    "noeunoeuf\n",
    "nosferalto\n",
    "nosferapti\n",
    "nostenfer\n",
    "nounourson\n",
    "nucleos\n",
    "nymphali\n",
    "obalie\n",
    "octillery\n",
    "ogerpon\n",
    "ohmassacre\n",
    "okeoke\n",
    "olivado\n",
    "olivini\n",
    "oniglali\n",
    "onix\n",
    "opermine\n",
    "oratoria\n",
    "ortide\n",
    "ossatueur\n",
    "osselait\n",
    "otaquin\n",
    "otaria\n",
    "otarlette\n",
    "ouistempo\n",
    "ouisticram\n",
    "ouvrifier\n",
    "oyacata\n",
    "pachirisu\n",
    "pachyradjah\n",
    "palarticho\n",
    "palkia\n",
    "palmaval\n",
    "pandarbare\n",
    "pandespiegle\n",
    "papilord\n",
    "papilusion\n",
    "papinox\n",
    "paragruel\n",
    "paras\n",
    "parasect\n",
    "parecool\n",
    "pashmilla\n",
    "passerouge\n",
    "patachiot\n",
    "paumedefer\n",
    "pechaminus\n",
    "pelagesable\n",
    "peregrain\n",
    "persian\n",
    "phanpy\n",
    "pharamp\n",
    "phione\n",
    "phogleur\n",
    "phyllali\n",
    "piafabec\n",
    "picassaut\n",
    "pichu\n",
    "piclairon\n",
    "pierroteknik\n",
    "pietace\n",
    "pifeuil\n",
    "pijako\n",
    "pikachu\n",
    "pimito\n",
    "pingoleon\n",
    "pitrouille\n",
    "plumeline\n",
    "pohm\n",
    "pohmarmotte\n",
    "pohmotte\n",
    "poichigeon\n",
    "poissirene\n",
    "poissoroy\n",
    "polagriffe\n",
    "polarhume\n",
    "polichombr\n",
    "poltchageist\n",
    "polthegeist\n",
    "pomdepik\n",
    "pomdorochi\n",
    "pomdramour\n",
    "pomdrapi\n",
    "ponchien\n",
    "ponchiot\n",
    "pondralugon\n",
    "ponyta\n",
    "porygon\n",
    "porygonz\n",
    "posipi\n",
    "poulpaf\n",
    "poussacha\n",
    "poussifeu\n",
    "predasterie\n",
    "prinplouf\n",
    "prismillon\n",
    "psykokwak\n",
    "psystigri\n",
    "ptera\n",
    "ptiravi\n",
    "ptitard\n",
    "ptyranidur\n",
    "pyrax\n",
    "pyrobut\n",
    "pyroli\n",
    "pyronille\n",
    "quartermac\n",
    "queulorior\n",
    "qulbutoke\n",
    "qwilfish\n",
    "qwilpik\n",
    "racaillou\n",
    "rafflesia\n",
    "raichu\n",
    "raikou\n",
    "ramboum\n",
    "ramoloss\n",
    "rampeailes\n",
    "rapasdepic\n",
    "rapion\n",
    "ratentif\n",
    "rattata\n",
    "rattatac\n",
    "rayquaza\n",
    "regice\n",
    "regidrago\n",
    "regieleki\n",
    "regigigas\n",
    "regirock\n",
    "registeel\n",
    "relicanth\n",
    "remoraid\n",
    "reptincel\n",
    "reshiram\n",
    "rexillius\n",
    "rhinastoc\n",
    "rhinocorne\n",
    "rhinoferos\n",
    "rhinolove\n",
    "riolu\n",
    "rocabot\n",
    "rocdefer\n",
    "roigada\n",
    "roitiflam\n",
    "rondoudou\n",
    "ronflex\n",
    "rongourmand\n",
    "rongrigou\n",
    "rosabyss\n",
    "roselia\n",
    "roserade\n",
    "rototaupe\n",
    "roublenard\n",
    "roucarnage\n",
    "roucool\n",
    "roucoups\n",
    "rouedefer\n",
    "roussil\n",
    "rozbouton\n",
    "rubombelle\n",
    "rugitlune\n",
    "sabelette\n",
    "sablaireau\n",
    "salameche\n",
    "salarsen\n",
    "sancoki\n",
    "sapereau\n",
    "saquedeneu\n",
    "sarmurai\n",
    "scalpereur\n",
    "scalpion\n",
    "scalproie\n",
    "scarabrute\n",
    "scarhino\n",
    "scobolide\n",
    "scolocendre\n",
    "scorplane\n",
    "scorvol\n",
    "scovilain\n",
    "scrutella\n",
    "seleroc\n",
    "selutin\n",
    "sepiatop\n",
    "sepiatroce\n",
    "seracrawl\n",
    "serpang\n",
    "serpenteeau\n",
    "seviper\n",
    "shaofouine\n",
    "sharpedo\n",
    "shaymin\n",
    "shifours\n",
    "siderella\n",
    "silvallie\n",
    "simiabraz\n",
    "simularbre\n",
    "sinistrail\n",
    "skelenox\n",
    "skitty\n",
    "smogo\n",
    "smogogo\n",
    "snubbull\n",
    "solaroc\n",
    "solgaleo\n",
    "solochi\n",
    "sonistrelle\n",
    "soporifik\n",
    "sorbebe\n",
    "sorbouboul\n",
    "sorboul\n",
    "sorcilence\n",
    "sovkipou\n",
    "spectreval\n",
    "spectrum\n",
    "spinda\n",
    "spiritomb\n",
    "spododo\n",
    "spoink\n",
    "stalgamin\n",
    "stari\n",
    "staross\n",
    "statitik\n",
    "steelix\n",
    "strassie\n",
    "sucreine\n",
    "sucroquin\n",
    "suicune\n",
    "sulfura\n",
    "superdofin\n",
    "sylveroy\n",
    "symbios\n",
    "tadmorv\n",
    "tagtag\n",
    "tapatoes\n",
    "tarenbulle\n",
    "tarinor\n",
    "tarinorme\n",
    "tarpaud\n",
    "tarsal\n",
    "tartard\n",
    "taupikeau\n",
    "taupiqueur\n",
    "tauros\n",
    "teddiursa\n",
    "tenefix\n",
    "tengalice\n",
    "tentacool\n",
    "tentacruel\n",
    "teraclope\n",
    "terapagos\n",
    "terhal\n",
    "terracool\n",
    "terracruel\n",
    "terraiste\n",
    "terrakium\n",
    "tetampoule\n",
    "tetarte\n",
    "tetesdefer\n",
    "theffroi\n",
    "theffroyable\n",
    "tiboudet\n",
    "tic\n",
    "tiplouf\n",
    "tissenboule\n",
    "togedemaru\n",
    "togekiss\n",
    "togepi\n",
    "togetic\n",
    "tokopisco\n",
    "tokopiyon\n",
    "tokorico\n",
    "tokotoro\n",
    "tomberro\n",
    "torgamord\n",
    "tortank\n",
    "torterra\n",
    "tortipouss\n",
    "toudoudou\n",
    "tournegrin\n",
    "tournicoton\n",
    "toutombe\n",
    "toxizap\n",
    "tranchodon\n",
    "trepassable\n",
    "triopikeau\n",
    "triopikeur\n",
    "trioxhydre\n",
    "tritonde\n",
    "tritosor\n",
    "tritox\n",
    "trompignon\n",
    "tropius\n",
    "trousselin\n",
    "tutafeh\n",
    "tutankafer\n",
    "tutetekri\n",
    "tygnon\n",
    "tylton\n",
    "type\n",
    "typhlosion\n",
    "tyranocif\n",
    "ursaking\n",
    "ursaring\n",
    "vacilys\n",
    "vaututrice\n",
    "vemini\n",
    "venalgue\n",
    "venipatte\n",
    "verpom\n",
    "vertdefer\n",
    "vibraninf\n",
    "victini\n",
    "vigoroth\n",
    "vipelierre\n",
    "virevorreur\n",
    "viridium\n",
    "virovent\n",
    "viskuse\n",
    "vivaldaim\n",
    "volcanion\n",
    "volcaropod\n",
    "voltali\n",
    "voltorbe\n",
    "voltoutou\n",
    "vorasterie\n",
    "vortente\n",
    "vostourno\n",
    "vrombi\n",
    "vrombotor\n",
    "wagomine\n",
    "wailmer\n",
    "wailord\n",
    "wattapik\n",
    "wattouat\n",
    "wimessir\n",
    "wushours\n",
    "xatu\n",
    "xerneas\n",
    "yanma\n",
    "yanmega\n",
    "ymphect\n",
    "yuyu\n",
    "yveltal\n",
    "zacian\n",
    "zamazenta\n",
    "zapetrel\n",
    "zarbi\n",
    "zarude\n",
    "zebibron\n",
    "zeblitz\n",
    "zekrom\n",
    "zeraora\n",
    "zeroid\n",
    "zigzaton\n",
    "zoroark\n",
    "zorua\n",
    "zygarde\n",
    "\"\"\"\n",
    "\n",
    "pokemons = [nom for nom in _POKEMON_DATA.strip().split(\"\\n\") if nom.strip()]\n",
    "\n",
    "# Quelques stats\n",
    "longueurs = [len(p) for p in pokemons]\n",
    "print(f\"Pokemon disponibles : {len(pokemons)}\")\n",
    "print(f\"Longueur moyenne : {sum(longueurs) / len(longueurs):.1f} lettres\")\n",
    "print(f\"Plus court : {min(longueurs)} lettres, plus long : {max(longueurs)} lettres\")\n",
    "print(f\"Exemples : {', '.join(pokemons[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": "---\n## Etape 2 : Preparer le modele\n\nOn reprend **exactement la meme architecture** que la lecon 5\n(embeddings + attention + MLP) avec les memes dimensions :\n\n| Parametre | Valeur | Rappel |\n|-----------|--------|--------|\n| Dimension embeddings | 16 | Chaque lettre = 16 nombres |\n| Taille MLP | 32 | Reseau de neurones interne |\n| Contexte | 8 | Fenetre de 8 lettres max |\n| Parametres | ~2 800 | Les nombres que le modele va ajuster |\n\nC'est exactement le modele de la lecon 5, mais cette fois on va\nl'entrainer **pour de vrai** !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vocabulaire ---\n",
    "VOCAB = list(\".abcdefghijklmnopqrstuvwxyz\")\n",
    "VOCAB_SIZE = len(VOCAB)  # 27\n",
    "char_to_id = {c: i for i, c in enumerate(VOCAB)}\n",
    "id_to_char = {i: c for i, c in enumerate(VOCAB)}\n",
    "\n",
    "# --- Configuration (memes dimensions que lecon 5) ---\n",
    "EMBED_DIM = 16\n",
    "CONTEXT = 8\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "nb_params = (\n",
    "    VOCAB_SIZE * EMBED_DIM  # tok_emb\n",
    "    + CONTEXT * EMBED_DIM  # pos_emb\n",
    "    + 3 * EMBED_DIM * EMBED_DIM  # Wq, Wk, Wv\n",
    "    + HIDDEN_DIM * EMBED_DIM  # W1\n",
    "    + HIDDEN_DIM  # b1\n",
    "    + EMBED_DIM * HIDDEN_DIM  # W2\n",
    "    + EMBED_DIM  # b2\n",
    "    + VOCAB_SIZE * EMBED_DIM  # W_out\n",
    ")\n",
    "\n",
    "# --- Fonctions utilitaires (memes que lecon 5) ---\n",
    "\n",
    "\n",
    "def rand_matrix(rows, cols, scale=0.3):\n",
    "    return [[random.gauss(0, scale) for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "\n",
    "def rand_vector(size, scale=0.3):\n",
    "    return [random.gauss(0, scale) for _ in range(size)]\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp(s - max_s) for s in scores]\n",
    "    total = sum(exps)\n",
    "    return [e / total for e in exps]\n",
    "\n",
    "\n",
    "def mat_vec(mat, vec):\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "\n",
    "# --- Initialisation des poids (aleatoires) ---\n",
    "tok_emb = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.5)\n",
    "pos_emb = rand_matrix(CONTEXT, EMBED_DIM, 0.5)\n",
    "Wq = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wk = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "Wv = rand_matrix(EMBED_DIM, EMBED_DIM, 0.2)\n",
    "W1 = rand_matrix(HIDDEN_DIM, EMBED_DIM, 0.2)\n",
    "b1 = rand_vector(HIDDEN_DIM, 0.1)\n",
    "W2 = rand_matrix(EMBED_DIM, HIDDEN_DIM, 0.2)\n",
    "b2 = rand_vector(EMBED_DIM, 0.1)\n",
    "W_out = rand_matrix(VOCAB_SIZE, EMBED_DIM, 0.2)\n",
    "\n",
    "print(f\"Mini-LLM initialise avec {nb_params} parametres aleatoires.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_avec_cache(sequence_ids):\n",
    "    \"\"\"Forward pass qui sauvegarde les etapes pour le backward.\"\"\"\n",
    "    n = len(sequence_ids)\n",
    "\n",
    "    # 1. Embeddings : token + position\n",
    "    hidden = []\n",
    "    for i, tok_id in enumerate(sequence_ids):\n",
    "        h = [tok_emb[tok_id][d] + pos_emb[i % CONTEXT][d] for d in range(EMBED_DIM)]\n",
    "        hidden.append(h)\n",
    "\n",
    "    # 2. Self-Attention (derniere position uniquement)\n",
    "    q = mat_vec(Wq, hidden[-1])\n",
    "\n",
    "    scores_bruts = []\n",
    "    cles = []\n",
    "    valeurs = []\n",
    "    for i in range(n):\n",
    "        k = mat_vec(Wk, hidden[i])\n",
    "        v = mat_vec(Wv, hidden[i])\n",
    "        score = sum(q[d] * k[d] for d in range(EMBED_DIM)) / math.sqrt(EMBED_DIM)\n",
    "        scores_bruts.append(score)\n",
    "        cles.append(k)\n",
    "        valeurs.append(v)\n",
    "\n",
    "    poids_attn = softmax(scores_bruts)\n",
    "\n",
    "    sortie_attn = [0.0] * EMBED_DIM\n",
    "    for i in range(n):\n",
    "        for d in range(EMBED_DIM):\n",
    "            sortie_attn[d] += poids_attn[i] * valeurs[i][d]\n",
    "\n",
    "    # Connexion residuelle 1\n",
    "    x = [hidden[-1][d] + sortie_attn[d] for d in range(EMBED_DIM)]\n",
    "    x_apres_attn = list(x)\n",
    "\n",
    "    # 3. MLP\n",
    "    h1_pre = [\n",
    "        sum(W1[j][d] * x[d] for d in range(EMBED_DIM)) + b1[j]\n",
    "        for j in range(HIDDEN_DIM)\n",
    "    ]\n",
    "    h1 = [max(0.0, v) for v in h1_pre]  # ReLU\n",
    "    sortie_mlp = [\n",
    "        sum(W2[d][j] * h1[j] for j in range(HIDDEN_DIM)) + b2[d]\n",
    "        for d in range(EMBED_DIM)\n",
    "    ]\n",
    "\n",
    "    # Connexion residuelle 2\n",
    "    x_final = [x[d] + sortie_mlp[d] for d in range(EMBED_DIM)]\n",
    "\n",
    "    # 4. Sortie\n",
    "    logits = [\n",
    "        sum(W_out[v][d] * x_final[d] for d in range(EMBED_DIM))\n",
    "        for v in range(VOCAB_SIZE)\n",
    "    ]\n",
    "    probas = softmax(logits)\n",
    "\n",
    "    cache = {\n",
    "        \"ids\": sequence_ids,\n",
    "        \"hidden\": hidden,\n",
    "        \"q\": q,\n",
    "        \"cles\": cles,\n",
    "        \"valeurs\": valeurs,\n",
    "        \"scores_bruts\": scores_bruts,\n",
    "        \"poids_attn\": poids_attn,\n",
    "        \"sortie_attn\": sortie_attn,\n",
    "        \"x_apres_attn\": x_apres_attn,\n",
    "        \"h1_pre\": h1_pre,\n",
    "        \"h1\": h1,\n",
    "        \"sortie_mlp\": sortie_mlp,\n",
    "        \"x_final\": x_final,\n",
    "    }\n",
    "    return probas, cache\n",
    "\n",
    "\n",
    "# Testons : loss initiale (poids aleatoires)\n",
    "loss_totale = 0\n",
    "nb = 0\n",
    "for pokemon in pokemons[:100]:  # 100 Pokemon pour aller vite\n",
    "    mot = \".\" + pokemon + \".\"\n",
    "    ids = [char_to_id[c] for c in mot]\n",
    "    for i in range(1, len(ids)):\n",
    "        seq = ids[:i][-CONTEXT:]\n",
    "        cible = ids[i]\n",
    "        probas, _ = forward_avec_cache(seq)\n",
    "        loss_totale += -math.log(probas[cible] + 1e-10)\n",
    "        nb += 1\n",
    "\n",
    "loss_initiale = loss_totale / nb\n",
    "print(f\"Loss initiale (poids aleatoires) : {loss_initiale:.3f}\")\n",
    "print(f\"Loss theorique d'un modele aleatoire : {math.log(VOCAB_SIZE):.3f}\")\n",
    "print(f\"  -> Le modele devine au hasard parmi {VOCAB_SIZE} lettres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer(debut=\".\", temperature=0.8, max_len=15):\n",
    "    \"\"\"Genere un nom de Pokemon lettre par lettre.\"\"\"\n",
    "    ids = [char_to_id[c] for c in debut]\n",
    "    resultat = debut\n",
    "    for _ in range(max_len):\n",
    "        probas, _ = forward_avec_cache(ids[-CONTEXT:])\n",
    "        if temperature != 1.0:\n",
    "            logits_t = [math.log(p + 1e-10) / temperature for p in probas]\n",
    "            probas = softmax(logits_t)\n",
    "        idx = random.choices(range(VOCAB_SIZE), weights=probas, k=1)[0]\n",
    "        if idx == char_to_id[\".\"]:\n",
    "            break\n",
    "        ids.append(idx)\n",
    "        resultat += id_to_char[idx]\n",
    "    return resultat[1:] if resultat.startswith(\".\") else resultat\n",
    "\n",
    "\n",
    "print(\"=== AVANT entrainement (poids aleatoires) ===\")\n",
    "print()\n",
    "noms_avant = []\n",
    "for _ in range(10):\n",
    "    nom = generer()\n",
    "    noms_avant.append(nom)\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "print()\n",
    "print(\"C'est du charabia ! Le modele ne sait pas ce qu'est un Pokemon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r7rlac2bv8",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 1)\n\nDans la cellule ci-dessous, change `ma_temperature` pour generer\ndes noms **avant** l'entrainement. Essaie `0.1` (tres sage) ou `2.0` (tres fou).\nTu verras que c'est du charabia dans tous les cas !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruk1hbg1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 1 : Change la temperature, puis Shift + Entree ---\n",
    "ma_temperature = 0.8  # <-- Essaie 0.1 (sage) ou 2.0 (fou) !\n",
    "\n",
    "print(f\"Generation AVANT entrainement (temperature = {ma_temperature}) :\")\n",
    "print()\n",
    "for _ in range(10):\n",
    "    nom = generer(temperature=ma_temperature)\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "print()\n",
    "print(\"C'est du charabia ! Sans entrainement, la temperature ne change rien.\")\n",
    "\n",
    "# Validation exercice 1\n",
    "verifier(\n",
    "    1,\n",
    "    ma_temperature != 0.8,\n",
    "    f\"Bien joue ! Avec temperature={ma_temperature}, les noms sont {'sages' if ma_temperature < 0.5 else 'fous' if ma_temperature > 1.5 else 'equilibres'}.\",\n",
    "    \"Change ma_temperature pour une autre valeur, par exemple 0.1 ou 2.0.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": "---\n## Etape 3 : La retropropagation\n\nDans les lecons 2 et 3, on calculait les gradients facilement parce que\nle modele etait simple. Maintenant, notre LLM a **7 couches de calcul** :\n\n```\nemb -> attention -> residuel -> MLP -> residuel -> W_out -> softmax\n```\n\nPour calculer les gradients, on fait le **chemin inverse** :\n\n```\nsoftmax -> W_out -> residuel -> MLP -> residuel -> attention -> emb\n```\n\nC'est la **retropropagation** (backpropagation). L'idee :\n\n1. On part de l'erreur a la sortie (la loss)\n2. On remonte couche par couche\n3. A chaque couche, on calcule \"de combien ce poids a contribue a l'erreur\"\n4. On ajuste chaque poids dans la bonne direction\n\n> **Analogie** : Imagine une chaine de dominos. Le dernier domino (la loss)\n> est tombe trop a droite. Tu remontes la chaine : quel domino a pousse\n> trop fort ? C'est celui-la qu'on ajuste.\n\nLa formule magique de la sortie est **exactement la meme** que dans les\nlecons 2 et 3 :\n\n```\ngradient[lettre] = proba_predite[lettre] - (1 si c'est la bonne reponse, 0 sinon)\n```\n\nEnsuite, ce gradient se propage en arriere a travers chaque couche."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(cache, probas, cible):\n",
    "    \"\"\"Calcule les gradients -- le chemin inverse du forward.\"\"\"\n",
    "    hidden = cache[\"hidden\"]\n",
    "    q = cache[\"q\"]\n",
    "    cles = cache[\"cles\"]\n",
    "    valeurs = cache[\"valeurs\"]\n",
    "    poids_attn = cache[\"poids_attn\"]\n",
    "    x_apres_attn = cache[\"x_apres_attn\"]\n",
    "    h1_pre = cache[\"h1_pre\"]\n",
    "    h1 = cache[\"h1\"]\n",
    "    x_final = cache[\"x_final\"]\n",
    "    ids = cache[\"ids\"]\n",
    "    n = len(ids)\n",
    "\n",
    "    # === ETAPE 1 : gradient de la sortie (cross-entropy + softmax) ===\n",
    "    # Meme formule que lecons 2 et 3 : augmente la bonne reponse, baisse les autres\n",
    "    d_logits = [probas[v] - (1.0 if v == cible else 0.0) for v in range(VOCAB_SIZE)]\n",
    "\n",
    "    # === ETAPE 2 : gradient de W_out ===\n",
    "    d_W_out = [\n",
    "        [d_logits[v] * x_final[d] for d in range(EMBED_DIM)] for v in range(VOCAB_SIZE)\n",
    "    ]\n",
    "    d_x = [\n",
    "        sum(d_logits[v] * W_out[v][d] for v in range(VOCAB_SIZE))\n",
    "        for d in range(EMBED_DIM)\n",
    "    ]\n",
    "\n",
    "    # === ETAPE 3 : connexion residuelle 2 -> gradient passe aux deux branches ===\n",
    "    d_mlp = list(d_x)\n",
    "    d_xa = list(d_x)\n",
    "\n",
    "    # === ETAPE 4 : backward du MLP ===\n",
    "    d_W2 = [[d_mlp[d] * h1[j] for j in range(HIDDEN_DIM)] for d in range(EMBED_DIM)]\n",
    "    d_b2 = list(d_mlp)\n",
    "    d_h1 = [\n",
    "        sum(d_mlp[d] * W2[d][j] for d in range(EMBED_DIM)) for j in range(HIDDEN_DIM)\n",
    "    ]\n",
    "    # ReLU backward : le gradient passe si h1_pre > 0, sinon bloque\n",
    "    d_h1p = [d_h1[j] * (1.0 if h1_pre[j] > 0 else 0.0) for j in range(HIDDEN_DIM)]\n",
    "\n",
    "    d_W1 = [\n",
    "        [d_h1p[j] * x_apres_attn[d] for d in range(EMBED_DIM)]\n",
    "        for j in range(HIDDEN_DIM)\n",
    "    ]\n",
    "    d_b1 = list(d_h1p)\n",
    "    for d in range(EMBED_DIM):\n",
    "        d_xa[d] += sum(d_h1p[j] * W1[j][d] for j in range(HIDDEN_DIM))\n",
    "\n",
    "    # === ETAPE 5 : connexion residuelle 1 ===\n",
    "    d_attn_out = list(d_xa)\n",
    "    d_hidden_last = list(d_xa)\n",
    "\n",
    "    # === ETAPE 6 : backward de l'attention ===\n",
    "    d_pw = [\n",
    "        sum(d_attn_out[d] * valeurs[i][d] for d in range(EMBED_DIM)) for i in range(n)\n",
    "    ]\n",
    "    d_val = [\n",
    "        [d_attn_out[d] * poids_attn[i] for d in range(EMBED_DIM)] for i in range(n)\n",
    "    ]\n",
    "    # Softmax backward\n",
    "    d_sc = [0.0] * n\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                d_sc[i] += poids_attn[i] * (1 - poids_attn[i]) * d_pw[i]\n",
    "            else:\n",
    "                d_sc[i] -= poids_attn[j] * poids_attn[i] * d_pw[j]\n",
    "\n",
    "    echelle = math.sqrt(EMBED_DIM)\n",
    "    d_sc = [ds / echelle for ds in d_sc]\n",
    "\n",
    "    d_q = [sum(d_sc[i] * cles[i][d] for i in range(n)) for d in range(EMBED_DIM)]\n",
    "    d_cles = [[d_sc[i] * q[d] for d in range(EMBED_DIM)] for i in range(n)]\n",
    "\n",
    "    d_Wq = [\n",
    "        [d_q[r] * hidden[-1][c] for c in range(EMBED_DIM)] for r in range(EMBED_DIM)\n",
    "    ]\n",
    "    for d in range(EMBED_DIM):\n",
    "        d_hidden_last[d] += sum(d_q[r] * Wq[r][d] for r in range(EMBED_DIM))\n",
    "\n",
    "    d_Wk = [[0.0] * EMBED_DIM for _ in range(EMBED_DIM)]\n",
    "    d_Wv = [[0.0] * EMBED_DIM for _ in range(EMBED_DIM)]\n",
    "    d_hkv = [[0.0] * EMBED_DIM for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for r in range(EMBED_DIM):\n",
    "            for c in range(EMBED_DIM):\n",
    "                d_Wk[r][c] += d_cles[i][r] * hidden[i][c]\n",
    "                d_Wv[r][c] += d_val[i][r] * hidden[i][c]\n",
    "                d_hkv[i][c] += d_cles[i][r] * Wk[r][c]\n",
    "                d_hkv[i][c] += d_val[i][r] * Wv[r][c]\n",
    "\n",
    "    # === ETAPE 7 : gradient des embeddings ===\n",
    "    d_tok_emb = [[0.0] * EMBED_DIM for _ in range(VOCAB_SIZE)]\n",
    "    d_pos_emb = [[0.0] * EMBED_DIM for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "        d_h = list(d_hkv[i])\n",
    "        if i == n - 1:\n",
    "            for d in range(EMBED_DIM):\n",
    "                d_h[d] += d_hidden_last[d]\n",
    "        for d in range(EMBED_DIM):\n",
    "            d_tok_emb[ids[i]][d] += d_h[d]\n",
    "            d_pos_emb[i][d] += d_h[d]\n",
    "\n",
    "    return {\n",
    "        \"d_W_out\": d_W_out,\n",
    "        \"d_W2\": d_W2,\n",
    "        \"d_b2\": d_b2,\n",
    "        \"d_W1\": d_W1,\n",
    "        \"d_b1\": d_b1,\n",
    "        \"d_Wq\": d_Wq,\n",
    "        \"d_Wk\": d_Wk,\n",
    "        \"d_Wv\": d_Wv,\n",
    "        \"d_tok_emb\": d_tok_emb,\n",
    "        \"d_pos_emb\": d_pos_emb,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Fonctions forward et backward definies !\")\n",
    "print(\n",
    "    \"7 etapes de retropropagation : sortie -> W_out -> MLP -> attention -> embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## Etape 4 : L'entrainement\n",
    "#\n",
    "# C'est la meme boucle que dans les lecons 2 et 3, mais avec le vrai LLM :\n",
    "#\n",
    "# 1. Prendre un Pokemon\n",
    "# 2. Pour chaque position, predire la lettre suivante\n",
    "# 3. Calculer l'erreur (la loss)\n",
    "# 4. Calculer les gradients (backward)\n",
    "# 5. Ajuster tous les poids un petit peu (SGD)\n",
    "# 6. Recommencer\n",
    "#\n",
    "# > **Cette cellule va tourner pendant ~1-2 minutes.**\n",
    "# > Pendant ce temps, lis les sections \"En vrai...\" plus bas !\n",
    "\n",
    "NB_EPOCHS = 10  # <-- Change cette valeur ! Essaie 5 (rapide) ou 20 (meilleur)\n",
    "vitesse = 0.01  # <-- Change cette valeur ! Essaie 0.005 (lent) ou 0.05 (rapide)\n",
    "\n",
    "positions_par_mot = sum(len(p) + 1 for p in pokemons) / len(pokemons)\n",
    "total_updates = int(NB_EPOCHS * len(pokemons) * positions_par_mot)\n",
    "\n",
    "print(f\"Entrainement : {NB_EPOCHS} epochs x {len(pokemons)} Pokemon\")\n",
    "print(f\"  ~{total_updates:,} mises a jour des poids au total\")\n",
    "print()\n",
    "\n",
    "_historique_loss_epochs = []\n",
    "\n",
    "debut_chrono = time.time()\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    random.shuffle(pokemons)\n",
    "    loss_epoch = 0\n",
    "    nb_epoch = 0\n",
    "\n",
    "    for idx_mot, pokemon in enumerate(pokemons):\n",
    "        mot = \".\" + pokemon + \".\"\n",
    "        ids = [char_to_id[c] for c in mot]\n",
    "\n",
    "        for i in range(1, len(ids)):\n",
    "            seq = ids[:i][-CONTEXT:]\n",
    "            cible = ids[i]\n",
    "\n",
    "            # Forward\n",
    "            probas, cache = forward_avec_cache(seq)\n",
    "            loss_epoch += -math.log(probas[cible] + 1e-10)\n",
    "            nb_epoch += 1\n",
    "\n",
    "            # Backward\n",
    "            grads = backward(cache, probas, cible)\n",
    "\n",
    "            # SGD : ajuster chaque poids\n",
    "            for v in range(VOCAB_SIZE):\n",
    "                for d in range(EMBED_DIM):\n",
    "                    W_out[v][d] -= vitesse * grads[\"d_W_out\"][v][d]\n",
    "            for j in range(HIDDEN_DIM):\n",
    "                b1[j] -= vitesse * grads[\"d_b1\"][j]\n",
    "                for d in range(EMBED_DIM):\n",
    "                    W1[j][d] -= vitesse * grads[\"d_W1\"][j][d]\n",
    "            for d in range(EMBED_DIM):\n",
    "                b2[d] -= vitesse * grads[\"d_b2\"][d]\n",
    "                for j in range(HIDDEN_DIM):\n",
    "                    W2[d][j] -= vitesse * grads[\"d_W2\"][d][j]\n",
    "            for r in range(EMBED_DIM):\n",
    "                for c in range(EMBED_DIM):\n",
    "                    Wq[r][c] -= vitesse * grads[\"d_Wq\"][r][c]\n",
    "                    Wk[r][c] -= vitesse * grads[\"d_Wk\"][r][c]\n",
    "                    Wv[r][c] -= vitesse * grads[\"d_Wv\"][r][c]\n",
    "            for tok_id in set(cache[\"ids\"]):\n",
    "                for d in range(EMBED_DIM):\n",
    "                    tok_emb[tok_id][d] -= vitesse * grads[\"d_tok_emb\"][tok_id][d]\n",
    "            for pos in range(len(seq)):\n",
    "                for d in range(EMBED_DIM):\n",
    "                    pos_emb[pos % CONTEXT][d] -= vitesse * grads[\"d_pos_emb\"][pos][d]\n",
    "\n",
    "        if (idx_mot + 1) % 250 == 0:\n",
    "            t = time.time() - debut_chrono\n",
    "            print(\n",
    "                f\"  Epoch {epoch + 1}/{NB_EPOCHS} | Mot {idx_mot + 1:>5}/{len(pokemons)} | Loss : {loss_epoch / nb_epoch:.3f} | {t:.0f}s\"\n",
    "            )\n",
    "\n",
    "    _historique_loss_epochs.append(loss_epoch / nb_epoch)\n",
    "\n",
    "    t = time.time() - debut_chrono\n",
    "    print(\n",
    "        f\"  === Epoch {epoch + 1} terminee | Loss : {loss_epoch / nb_epoch:.3f} | {t:.0f}s ===\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "duree = time.time() - debut_chrono\n",
    "print(f\"Entrainement termine en {duree:.0f} secondes !\")\n",
    "print(f\"Loss : {loss_initiale:.3f} -> {loss_epoch / nb_epoch:.3f}\")\n",
    "\n",
    "# Visualisation de la courbe de loss\n",
    "afficher_evolution_loss(_historique_loss_epochs, titre=\"Evolution de la loss par epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n9i4z2sneie",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 2)\n\nObserve le resultat de l'entrainement ci-dessus :\n- Combien de temps a-t-il fallu par epoch ?\n- La loss a-t-elle bien baisse ?\n\nSi tu veux, re-execute la cellule d'entrainement en changeant\n`NB_EPOCHS` (5 = rapide, 20 = meilleur) ou `vitesse` (0.005, 0.05).\nAttention : il faudra re-executer les cellules d'initialisation aussi !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlzojvdnrxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 2 : Observe les resultats, puis Shift + Entree ---\n",
    "\n",
    "print(\"Resultats de l'entrainement :\")\n",
    "print(f\"  Epochs : {NB_EPOCHS}\")\n",
    "print(f\"  Vitesse : {vitesse}\")\n",
    "print(f\"  Duree totale : {duree:.0f} secondes ({duree / NB_EPOCHS:.0f}s par epoch)\")\n",
    "print(f\"  Loss initiale : {loss_initiale:.3f}\")\n",
    "print(f\"  Loss finale : {loss_epoch / nb_epoch:.3f}\")\n",
    "print(f\"  Amelioration : {(1 - (loss_epoch / nb_epoch) / loss_initiale) * 100:.0f}%\")\n",
    "print()\n",
    "if loss_epoch / nb_epoch < 2.5:\n",
    "    print(\"Le modele a bien appris !\")\n",
    "else:\n",
    "    print(\"Le modele peut encore s'ameliorer. Essaie plus d'epochs !\")\n",
    "\n",
    "# Validation exercice 2\n",
    "verifier(\n",
    "    2,\n",
    "    loss_epoch / nb_epoch < loss_initiale,\n",
    "    f\"Bien observe ! La loss est passee de {loss_initiale:.2f} a {loss_epoch / nb_epoch:.2f} en {NB_EPOCHS} epochs.\",\n",
    "    \"Re-execute l'entrainement si la loss n'a pas baisse.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": "---\n## En vrai... pendant que le modele s'entraine\n\n### Autograd vs notre backward manuel\n\nOn a ecrit ~60 lignes de code pour le backward pass. C'est beaucoup !\n\nEn vrai, les frameworks comme **PyTorch** font ca **automatiquement**.\nTu ecris juste le forward pass, et PyTorch calcule les gradients tout seul.\nCa s'appelle **l'autograd** (differentiation automatique).\n\nC'est exactement ce que fait `microgpt.py` de Karpathy : il definit\ndes operations (`+`, `*`, `exp`) qui \"se souviennent\" comment elles ont\nete calculees, puis il remonte la chaine automatiquement.\n\n### GPU vs CPU\n\nNotre boucle Python fait les calculs **un par un**. Chaque multiplication,\nchaque addition, une a la fois.\n\nUn **GPU** (la carte graphique de ton PC) peut faire **des milliers de\nmultiplications en parallele**. C'est comme la difference entre :\n- Un cuisinier qui prepare les plats un par un (CPU)\n- Une brigade de 1000 cuisiniers qui preparent tous en meme temps (GPU)\n\nC'est pour ca que l'entrainement de GPT-4 a utilise **25 000 GPU**\npendant **plusieurs mois**. Notre mini-LLM, avec ses ~2 800 parametres,\ns'entraine en quelques minutes sur un seul CPU.\n\n### Adam vs SGD\n\nOn utilise la descente de gradient la plus simple : **SGD** (Stochastic\nGradient Descent). A chaque pas, on corrige d'un montant fixe.\n\n**Adam** est un optimiseur plus intelligent :\n- Il **accelere** dans les zones plates (quand les gradients sont petits)\n- Il **freine** dans les zones pentues (quand les gradients sont grands)\n- Il se souvient des gradients precedents pour mieux orienter la correction\n\nC'est comme un velo avec des vitesses : tu adaptes ton effort au terrain.\nPresque tous les vrais LLM utilisent Adam (ou une variante comme AdamW)."
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "source": "---\n## Ce qu'on n'a pas implémenté\n\nNotre mini-LLM est **fonctionnel** mais simplifié. Voici ce que les vrais\nLLM ajoutent :\n\n| Technique | Notre mini-LLM | Les vrais LLM |\n|-----------|----------------|---------------|\n| **Batching** | 1 mot à la fois | 64-512 mots en parallèle |\n| **LayerNorm** | Non | Oui (stabilise l'entraînement) |\n| **Dropout** | Non | Oui (évite le sur-apprentissage) |\n| **Multi-head** | 1 tête | 4-96 têtes en parallèle |\n| **Multi-couches** | 1 couche | 6-96 couches empilées |\n| **Optimizer** | SGD basique | Adam (plus intelligent) |\n| **GPU** | Non (Python pur) | Oui (1000x plus rapide) |\n\nMais l'**algorithme** est le même ! La différence, c'est l'**échelle**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== APRES entrainement ===\")\n",
    "print()\n",
    "noms_apres = []\n",
    "for _ in range(15):\n",
    "    nom = generer(temperature=0.8)\n",
    "    noms_apres.append(nom)\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "\n",
    "print()\n",
    "print(\"--- Comparaison ---\")\n",
    "print()\n",
    "print(\"AVANT (charabia) :\")\n",
    "for nom in noms_avant[:5]:\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "print()\n",
    "print(\"APRES (ca ressemble a des Pokemon !) :\")\n",
    "for nom in noms_apres[:5]:\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "\n",
    "# Visualisation des predictions apres entrainement\n",
    "_probas_post, _ = forward_avec_cache([char_to_id[c] for c in \".pik\"][-CONTEXT:])\n",
    "_top = sorted(range(VOCAB_SIZE), key=lambda i: -_probas_post[i])[:5]\n",
    "afficher_barres(\n",
    "    [_probas_post[i] for i in _top],\n",
    "    [id_to_char[i] for i in _top],\n",
    "    titre=\"Top 5 predictions apres '.pik' (apres entrainement)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1hihla757d",
   "metadata": {},
   "source": "---\n### A toi de jouer ! (Exercice 3)\n\nMaintenant que le modele est entraine, change `ma_temperature` et\n`mon_debut` pour explorer ce qu'il a appris.\n- Temperature `0.1` : noms \"sages\" et repetitifs\n- Temperature `2.0` : noms \"fous\" et originaux\n- Debut `\".pik\"` : force le modele a continuer apres \"pik\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19t34hux36j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXERCICE 3 : Change la temperature et le debut, puis Shift + Entree ---\n",
    "ma_temperature = 0.8  # <-- Essaie 0.1 (sage) ou 2.0 (fou) !\n",
    "mon_debut = \".\"  # <-- Essaie \".pik\", \".bul\" ou \".dra\" !\n",
    "\n",
    "print(\n",
    "    f\"Generation APRES entrainement (temperature={ma_temperature}, debut='{mon_debut}') :\"\n",
    ")\n",
    "print()\n",
    "for _ in range(15):\n",
    "    nom = generer(debut=mon_debut, temperature=ma_temperature)\n",
    "    print(f\"  {nom.capitalize()}\")\n",
    "print()\n",
    "print(\"Compare avec l'exercice 1 : maintenant le modele sait ce qu'est un Pokemon !\")\n",
    "\n",
    "# Validation exercice 3\n",
    "verifier(\n",
    "    3,\n",
    "    ma_temperature != 0.8 or mon_debut != \".\",\n",
    "    f\"Genial ! Generation avec temperature={ma_temperature} et debut='{mon_debut}'.\",\n",
    "    \"Change ma_temperature ou mon_debut pour explorer le modele entraine.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "source": "---\n## Ce qu'on a appris\n\n```\nLecon 1 : Compter les lettres qui suivent        -> bigramme\nLecon 2 : Apprendre de ses erreurs               -> entrainement\nLecon 3 : Regarder plusieurs lettres en arriere   -> embeddings + contexte\nLecon 4 : Choisir les lettres importantes          -> attention\nLecon 5 : Assembler le tout                       -> mini-LLM\nLecon 6 : Entrainer pour de vrai                  -> retropropagation !\n```\n\n### Ce qu'on a fait dans cette lecon\n\n1. **Charge ~1 000 noms de Pokemon**\n2. **Implemente la retropropagation** : 7 etapes pour remonter les gradients\n3. **Entraine le mini-LLM** avec la descente de gradient (SGD)\n4. **Genere des noms de Pokemon inventes** qui ressemblent a de vrais Pokemon\n\n### Ce qu'on a appris\n\n- La **retropropagation** remonte l'erreur couche par couche\n- L'**entrainement** = repeter forward + backward + mise a jour des poids\n- Meme un modele de ~2 800 parametres peut **apprendre des patterns**\n- La difference avec ChatGPT n'est pas l'algorithme, c'est **l'echelle**\n\n---\n*Tu as construit et entraine ton propre LLM. Felicitations !*"
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources (ISO 42001)\n",
    "\n",
    "- **Retropropagation et descente de gradient** : [microgpt.py](https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95) -- Andrej Karpathy, implementation complete du backward pass\n",
    "- **Architecture GPT (embedding + attention + MLP)** : [Video \"Let's build GPT\"](https://www.youtube.com/watch?v=kCc8FmEb1nY) -- Andrej Karpathy (2023)\n",
    "- **Cross-entropy loss et gradient softmax** : [3Blue1Brown - Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) -- Grant Sanderson\n",
    "- **\"Attention Is All You Need\"** : Vaswani et al., 2017, [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "- **Donnees d'entrainement** : [PokeAPI](https://pokeapi.co/) -- (c) Nintendo / Creatures Inc. / GAME FREAK inc., usage educatif uniquement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
